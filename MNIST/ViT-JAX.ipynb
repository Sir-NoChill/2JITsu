{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a873308e-8a53-4952-ac39-3d00ea2fb72c",
   "metadata": {},
   "source": [
    "# Implementation of ViT\n",
    "\n",
    "This one is meant to be simple so that I can play with parameters. Taken from [Dive Into Deep Learning chapter 11](https://d2l.ai/chapter_attention-mechanisms-and-transformers/vision-transformer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "722ac94c-bbe4-497b-91e8-cb53cf15e4b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-14 21:31:58.140432: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.5 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import jax\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "\n",
    "key = jax.random.key(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b9d6cd2-c473-4d66-8aa4-85c4f3f4fd62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_shape(a, b):\n",
    "    if isinstance(a, tuple):\n",
    "        return a == b\n",
    "    for i in a:\n",
    "        if a[i] == b[i]:\n",
    "            continue\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fdf3e66e-3a56-43f0-b58d-4b7715cfc497",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DotProductAttention(nn.Module):\n",
    "    \"\"\"Scaled dot product attention.\n",
    "    https://d2l.ai/chapter_attention-mechanisms-and-transformers/\n",
    "    attention-scoring-functions.html\n",
    "    \"\"\"\n",
    "    dropout: float\n",
    "\n",
    "    # Shape of queries: (batch_size, no. of queries, d)\n",
    "    # Shape of keys: (batch_size, no. of key-value pairs, d)\n",
    "    # Shape of values: (batch_size, no. of key-value pairs, value dimension)\n",
    "    # Shape of valid_lens: (batch_size,) or (batch_size, no. of queries)\n",
    "    @nn.compact\n",
    "    def __call__(self, queries, keys, values, valid_lens=None,\n",
    "                 training=False):\n",
    "        d = queries.shape[-1]\n",
    "        # Swap the last two dimensions of keys with keys.swapaxes(1, 2)\n",
    "        scores = (queries@(keys.swapaxes(2,3))) / jnp.sqrt(d)\n",
    "        attention_weights = nn.softmax(scores, valid_lens)\n",
    "        dropout_layer = nn.Dropout(self.dropout, deterministic=not training)\n",
    "        return dropout_layer(attention_weights)@values, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ea02c46f-94e0-48dd-b31d-ac21fd3cefd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Borrowed from\n",
    "    https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks\n",
    "    /JAX/tutorial6/Transformers_and_MHAttention.html\n",
    "\n",
    "    and \n",
    "\n",
    "    https://github.com/d2l-ai/d2l-en\n",
    "    /blob/23d7a5aecceee57d1292c56e90cce307f183bb0a/d2l/jax.py\n",
    "    \"\"\"\n",
    "    embed_dim: int\n",
    "    num_heads: int\n",
    "    dropout: float\n",
    "    use_bias: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        self.weight_projection = nn.Dense(\n",
    "            3 * self.embed_dim,\n",
    "            # see https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros\n",
    "        )\n",
    "        self.offset_projection = nn.Dense(\n",
    "            self.embed_dim,\n",
    "            kernel_init=nn.initializers.xavier_uniform(),\n",
    "            bias_init=nn.initializers.zeros,\n",
    "            use_bias=self.use_bias\n",
    "        )\n",
    "        self.attention = DotProductAttention(self.dropout)\n",
    "\n",
    "    def __call__(self, x, mask=None, training=False):\n",
    "        batch_size, sequence_length, embed_dim = x.shape\n",
    "        if mask is not None:\n",
    "            mask = expand_mask(mask)\n",
    "        stacked_weights = self.weight_projection(x)\n",
    "\n",
    "        # Seperate the weights from linear outputs\n",
    "        # weights (QKV)\n",
    "        stacked_weights = stacked_weights.reshape(batch_size, sequence_length, self.num_heads, -1)\n",
    "        # transpose to [batch, head, sequence_length, dimensions]\n",
    "        stacked_weights = stacked_weights.transpose(0, 2, 1, 3)\n",
    "        q, k, v = jnp.array_split(stacked_weights, 3, axis=-1)\n",
    "\n",
    "        # Determine the outputs\n",
    "        values, attention = self.attention(q, k, v)\n",
    "        # transpose to [batch, sequence_length, head, dimensions]\n",
    "        values = values.transpose(0, 2, 1, 3)\n",
    "        values = values.reshape(batch_size, sequence_length, embed_dim)\n",
    "        o = self.offset_projection(values)\n",
    "\n",
    "        return o, attention\n",
    "\n",
    "\n",
    "# Check the implementation of MHA\n",
    "this_key, key = jax.random.split(key)\n",
    "x = jax.random.normal(this_key, (3, 16, 128))\n",
    "MHA = MultiHeadAttention(embed_dim=128, num_heads=4, dropout=0.5)\n",
    "this_key, key = jax.random.split(key)\n",
    "params = MHA.init(this_key, x)['params']\n",
    "out, attention = MHA.apply({'params': params}, x)\n",
    "\n",
    "assert check_shape(out.shape, x.shape)\n",
    "assert check_shape(attention.shape, (3, 4, 16, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d88830-ac8c-49c9-864c-1f9df9f229b8",
   "metadata": {},
   "source": [
    "## Patch Embedding\n",
    "\n",
    "Split the image into patches, then linearly project the flattened patches. AKA convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0e73d25a-f5d2-4686-8c71-fa977d19f849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    img_size: int = 96\n",
    "    patch_size: int = 16\n",
    "    num_hiddens: int = 512\n",
    "\n",
    "    def setup(self):\n",
    "        def _make_tuple(x):\n",
    "            if not isinstance(x, (list, tuple)):\n",
    "                return (x, x)\n",
    "            return x\n",
    "\n",
    "        img_size, patch_size = _make_tuple(self.img_size), _make_tuple(self.patch_size)\n",
    "        self.num_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
    "\n",
    "        self.conv = nn.Conv(self.num_hiddens, kernel_size=patch_size,\n",
    "                           strides=patch_size, padding='SAME')\n",
    "\n",
    "    def __call__(self, X):\n",
    "        X = self.conv(X)\n",
    "        return X.reshape((X.shape[0], -1, X.shape[3]))\n",
    "\n",
    "\n",
    "# Check the implementation of the patch embedding\n",
    "img_size, patch_size, num_hiddens, batch_size = 96, 16, 512, 4\n",
    "patch_emb = PatchEmbedding(img_size, patch_size, num_hiddens)\n",
    "X = jnp.zeros((batch_size, img_size, img_size, 3))\n",
    "this_key, key = jax.random.split(key)\n",
    "output, _ = patch_emb.init_with_output(this_key, X)\n",
    "\n",
    "assert check_shape((batch_size, (img_size//patch_size)**2, num_hiddens), output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7484529e-fafb-4af9-8c7f-83e69963757e",
   "metadata": {},
   "source": [
    "## ViT Encoder Stage\n",
    "\n",
    "Normalization occurs before the multi-head attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "3468c87e-160e-43f2-8d35-4664d802cc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViTMLP(nn.Module):\n",
    "    mlp_num_hiddens: int\n",
    "    mlp_num_outputs: int\n",
    "    dropout: float = 0.5\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, training=False):\n",
    "        x = nn.Dense(self.mlp_num_hiddens)(x)\n",
    "        x = nn.gelu(x)\n",
    "        x = nn.Dropout(self.dropout, deterministic=not training)(x)\n",
    "        x = nn.Dense(self.mlp_num_outputs)(x)\n",
    "        x = nn.Dropout(self.dropout, deterministic=not training)(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ViTBlock(nn.Module):\n",
    "    num_hiddens: int\n",
    "    mlp_num_hiddens: int\n",
    "    num_heads: int\n",
    "    dropout: float\n",
    "    use_bias: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        self.attention = MultiHeadAttention(self.num_hiddens, self.num_heads,\n",
    "                                            self.dropout, self.use_bias)\n",
    "        self.mlp = ViTMLP(self.mlp_num_hiddens, self.num_hiddens, self.dropout)\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x, valid_lens=None, training=False):\n",
    "        x = x + self.attention(*([nn.LayerNorm()(x)]),\n",
    "                               valid_lens)[0]\n",
    "        return x + self.mlp(nn.LayerNorm()(x), training)\n",
    "\n",
    "\n",
    "x = jnp.ones((2, 100, 24))\n",
    "encoder_blk = ViTBlock(24, 48, 8, 0.5)\n",
    "this_key, key = jax.random.split(key)\n",
    "assert check_shape(encoder_blk.init_with_output(this_key, x)[0].shape, x.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5e3c13-5f42-4673-a1a8-ab79ec892383",
   "metadata": {},
   "source": [
    "## Complete ViT implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6f78ae3e-0a56-4d09-ad5e-e4bce339c38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"\n",
    "    Vision Transformer\n",
    "    \"\"\"\n",
    "    image_size: int\n",
    "    patch_size: int\n",
    "    num_hiddens: int\n",
    "    mlp_num_hiddens: int\n",
    "    num_heads: int\n",
    "    num_blks: int\n",
    "    emb_dropout: float\n",
    "    blk_dropout: float\n",
    "    lr: float = 0.1\n",
    "    use_bias: bool = False\n",
    "    num_classes: int = 10\n",
    "    training: bool = False\n",
    "\n",
    "    def setup(self):\n",
    "        self.patch_embedding = PatchEmbedding(self.img_size, self.patch_size, self.num_hiddens)\n",
    "        self.cls_token = self.param('cls_token', nn.initializers.zeros, 1, 1, self.num_hiddens)\n",
    "\n",
    "        num_steps = self.patch_embedding.num_patches + 1\n",
    "\n",
    "        # Positional Embeddings\n",
    "        self.pos_embedding = self.param('pos_embed', nn.initializers.normal(), 1, num_steps, self.num_hiddens)\n",
    "        self.blks = [ViTBlock(self.num_hiddens, self.mlp_num_hiddens, self.num_heads, self.blk_dropout, self.use_bias)\n",
    "                   for _ in range(self.num_blks)]\n",
    "        self.head = nn.Sequential([nn.LayerNorm(), nn.Dense(self.num_classes)])\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = self.patch_embedding(x)\n",
    "        x = jnp.concatenate((jnp.tile(self.cls_token, (x.shape[0], 1, 1)), x), 1)\n",
    "        x = nn.Dropout(self.emb_dropout, deterministic=not self.training)(x + self.pos_embedding)\n",
    "        for blk in self.blks:\n",
    "            x = blk(x, training=self.training)\n",
    "        return self.head(x[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65a9c7e-0720-4a8b-ab48-6b6ddae029a5",
   "metadata": {},
   "source": [
    "## JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446f0052-5704-46e6-93f0-cc1c4c262eb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40af714d-cd86-4c83-a216-291b2fd90ef9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb469c3-5d7d-4148-b529-068387a4af9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 96\n",
    "patch_size = img_size / 6\n",
    "num_hiddens, mlp_num_hiddens, num_heads, num_blks = 512, 2048, 8, 2\n",
    "emb_dropout, blk_dropout, lr = 0.1, 0.1, 0.1\n",
    "model = ViT(img_size, patch_size, num_hiddens, mlp_num_hiddens,\n",
    "            num_heads, num_blks, emb_dropout, blk_dropout, lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9feb8b-951d-4ef4-bc2e-0fa4c9ab5ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# PyTorch TensorBoard support\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Create datasets for training & validation, download if necessary\n",
    "training_set = torchvision.datasets.FashionMNIST('./data', train=True, transform=transform, download=True)\n",
    "validation_set = torchvision.datasets.FashionMNIST('./data', train=False, transform=transform, download=True)\n",
    "\n",
    "# Create data loaders for our datasets; shuffle for training, not for validation\n",
    "training_loader = torch.utils.data.DataLoader(training_set, batch_size=4, shuffle=True)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_set, batch_size=4, shuffle=False)\n",
    "\n",
    "# Class labels\n",
    "classes = ('T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "        'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3daf174f-9cb3-4402-9776-ffed1acc6687",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a2708853-e269-48ec-8ad0-fa1deda26bb0",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1a37bb-8bba-41c8-a33e-f8dc9838f505",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
