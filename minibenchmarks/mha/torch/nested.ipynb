{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f87cb309-8904-4b6d-9ffe-1a2b5be6f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "302c44cb-a27d-45de-95cc-3051c4bb7523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes multi-head attention. Supports nested or padded tensors.\n",
    "\n",
    "    Args:\n",
    "        E_q (int): Size of embedding dim for query\n",
    "        E_k (int): Size of embedding dim for key\n",
    "        E_v (int): Size of embedding dim for value\n",
    "        E_total (int): Total embedding dim of combined heads post input projection. Each head\n",
    "            has dim E_total // nheads\n",
    "        nheads (int): Number of heads\n",
    "        dropout_p (float, optional): Dropout probability. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, E_q: int, E_k: int, E_v: int, E_total: int,\n",
    "                 nheads: int, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.query_proj = nn.Linear(E_q, E_total)\n",
    "        self.key_proj = nn.Linear(E_k, E_total)\n",
    "        self.value_proj = nn.Linear(E_v, E_total)\n",
    "        E_out = E_q\n",
    "        self.out_proj = nn.Linear(E_total, E_out)\n",
    "        assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.E_head = E_total // nheads\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass; runs the following process:\n",
    "            1. Apply input projection\n",
    "            2. Split heads and prepare for SDPA\n",
    "            3. Run SDPA\n",
    "            4. Apply output projection\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): query of shape (N, L_t, E_q)\n",
    "            key (torch.Tensor): key of shape (N, L_s, E_k)\n",
    "            value (torch.Tensor): value of shape (N, L_s, E_v)\n",
    "\n",
    "        Returns:\n",
    "            attn_output (torch.Tensor): output of shape (N, L_t, E_q)\n",
    "        \"\"\"\n",
    "        # Step 1. Apply input projection\n",
    "        # TODO: demonstrate packed projection\n",
    "        query = self.query_proj(query)\n",
    "        key = self.key_proj(key)\n",
    "        value = self.value_proj(value)\n",
    "\n",
    "        # Step 2. Split heads and prepare for SDPA\n",
    "        # reshape query, key, value to separate by head\n",
    "        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n",
    "        query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "\n",
    "        # Step 3. Run SDPA\n",
    "        # (N, nheads, L_t, E_head)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query, key, value, dropout_p=dropout_p, is_causal=True)\n",
    "        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
    "\n",
    "        # Step 4. Apply output projection\n",
    "        # (N, L_t, E_total) -> (N, L_t, E_out)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fac66469-c9e3-48e6-a776-1ca13d71dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 512\n",
    "E_q, E_k, E_v, E_total = 512, 512, 512, 512\n",
    "E_out = E_q\n",
    "nheads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aa16f94-f66c-4f2e-a637-c5464bd4ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6104bb2-48ca-45f4-b011-921efe5ff52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_sentence_lengths(alpha: float, batch_size: int) -> torch.Tensor:\n",
    "    # generate fake corpus by unigram Zipf distribution\n",
    "    # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858\n",
    "    sentence_lengths = np.empty(batch_size, dtype=int)\n",
    "    for ibatch in range(batch_size):\n",
    "        sentence_lengths[ibatch] = 1\n",
    "        word = np.random.zipf(alpha)\n",
    "        while word != 3 and word != 386 and word != 858:\n",
    "            sentence_lengths[ibatch] += 1\n",
    "            word = np.random.zipf(alpha)\n",
    "    return torch.tensor(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "40bca0e3-cc7a-4a04-916e-3644b7aeb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(N, E_q, E_k, E_v, device):\n",
    "    # generate semi-realistic data using Zipf distribution for sentence lengths\n",
    "    sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N)\n",
    "\n",
    "    # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged\n",
    "    # dimension and works with torch.compile. The batch items each have shape (B, S*, D)\n",
    "    # where B = batch size, S* = ragged sequence length, and D = embedding dimension.\n",
    "    query = torch.nested.nested_tensor([\n",
    "        torch.randn(l.item(), E_q, device=device)\n",
    "        for l in sentence_lengths\n",
    "    ], layout=torch.jagged)\n",
    "\n",
    "    key = torch.nested.nested_tensor([\n",
    "        torch.randn(s.item(), E_k, device=device)\n",
    "        for s in sentence_lengths\n",
    "    ], layout=torch.jagged)\n",
    "\n",
    "    value = torch.nested.nested_tensor([\n",
    "        torch.randn(s.item(), E_v, device=device)\n",
    "        for s in sentence_lengths\n",
    "    ], layout=torch.jagged)\n",
    "\n",
    "    return query, key, value, sentence_lengths\n",
    "\n",
    "query, key, value, sentence_lengths = gen_batch(N, E_q, E_k, E_v, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d818df1e-d72f-4858-a793-6c0bff3437bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/seliayeu/mambaforge/envs/2JITsu/lib/python3.11/site-packages/torch/nested/__init__.py:226: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at ../aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  return _nested.nested_tensor(\n"
     ]
    }
   ],
   "source": [
    "def jagged_to_padded(jt, padding_val):\n",
    "    # TODO: do jagged -> padded directly when this is supported\n",
    "    return torch.nested.to_padded_tensor(\n",
    "        torch.nested.nested_tensor(list(jt.unbind())),\n",
    "        padding_val)\n",
    "\n",
    "padded_query, padded_key, padded_value = (\n",
    "    jagged_to_padded(t, 0.0) for t in (query, key, value)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71664dc7-d949-4a47-93f7-b2f0a21f6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(E_q, E_k, E_v, E_total, nheads, dropout_p).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b7783769-77ad-48b4-b909-7e7205c50719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== without torch.compile ===\n",
      "nested and padded calculations differ by 0.0\n",
      "nested tensor multi-head attention takes 0.014209666289389133 seconds\n",
      "padded tensor multi-head attention takes 0.009372099302709103 seconds\n",
      "=== with torch.compile ===\n",
      "nested and padded calculations differ by 0.0\n",
      "nested tensor multi-head attention takes 0.001617523841559887 seconds\n",
      "padded tensor multi-head attention takes 0.009139347821474075 seconds\n"
     ]
    }
   ],
   "source": [
    "def benchmark(func, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    begin = timeit.default_timer()\n",
    "    output = func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    end = timeit.default_timer()\n",
    "    return output, (end - begin)\n",
    "\n",
    "output_nested, time_nested = benchmark(mha, query, key, value)\n",
    "output_padded, time_padded = benchmark(mha, padded_query, padded_key, padded_value)\n",
    "\n",
    "# padding-specific step: remove output projection bias from padded entries for fair comparison\n",
    "for i, entry_length in enumerate(sentence_lengths):\n",
    "    output_padded[i, entry_length:] = 0.0\n",
    "\n",
    "print(\"=== without torch.compile ===\")\n",
    "print(\"nested and padded calculations differ by\", (jagged_to_padded(output_nested, 0.0) - output_padded).abs().max().item())\n",
    "print(\"nested tensor multi-head attention takes\", time_nested, \"seconds\")\n",
    "print(\"padded tensor multi-head attention takes\", time_padded, \"seconds\")\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha = torch.compile(mha)\n",
    "compiled_mha(query, key, value)\n",
    "# ...now benchmark\n",
    "compiled_output_nested, compiled_time_nested = benchmark(\n",
    "    compiled_mha, query, key, value)\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha(padded_query, padded_key, padded_value)\n",
    "# ...now benchmark\n",
    "compiled_output_padded, compiled_time_padded = benchmark(\n",
    "    compiled_mha, padded_query, padded_key, padded_value)\n",
    "\n",
    "# padding-specific step: remove output projection bias from padded entries for fair comparison\n",
    "for i, entry_length in enumerate(sentence_lengths):\n",
    "    compiled_output_padded[i, entry_length:] = 0.0\n",
    "\n",
    "print(\"=== with torch.compile ===\")\n",
    "print(\"nested and padded calculations differ by\", (jagged_to_padded(compiled_output_nested, 0.0) - compiled_output_padded).abs().max().item())\n",
    "print(\"nested tensor multi-head attention takes\", compiled_time_nested, \"seconds\")\n",
    "print(\"padded tensor multi-head attention takes\", compiled_time_padded, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c55778a-2ee3-4764-bc8a-b9318c374962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5005,  3.0300, -5.8522])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n",
    "x, y = torch.randn(3, 5), torch.randn(3, 5)\n",
    "batched_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e783f-db2e-4a33-875e-63c2003f774f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
