{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aab991a0-2a13-4ec1-b401-e54bcde95488",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Minibenchmark\n",
    "This benchmark is taken from [this Pytorch tutorial](https://pytorch.org/tutorials/prototype/nestedtensor.html).\n",
    "\n",
    "This benchmark expects packages from the `requirements.txt` in the root directory to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f87cb309-8904-4b6d-9ffe-1a2b5be6f483",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tqdm\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "302c44cb-a27d-45de-95cc-3051c4bb7523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes multi-head attention. Supports nested or padded tensors.\n",
    "\n",
    "    Args:\n",
    "        E_q (int): Size of embedding dim for query\n",
    "        E_k (int): Size of embedding dim for key\n",
    "        E_v (int): Size of embedding dim for value\n",
    "        E_total (int): Total embedding dim of combined heads post input projection. Each head\n",
    "            has dim E_total // nheads\n",
    "        nheads (int): Number of heads\n",
    "        dropout_p (float, optional): Dropout probability. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, E_q: int, E_k: int, E_v: int, E_total: int,\n",
    "                 nheads: int):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.query_proj = nn.Linear(E_q, E_total)\n",
    "        self.key_proj = nn.Linear(E_k, E_total)\n",
    "        self.value_proj = nn.Linear(E_v, E_total)\n",
    "        E_out = E_q\n",
    "        self.out_proj = nn.Linear(E_total, E_out)\n",
    "        assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.E_head = E_total // nheads\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass; runs the following process:\n",
    "            1. Apply input projection\n",
    "            2. Split heads and prepare for SDPA\n",
    "            3. Run SDPA\n",
    "            4. Apply output projection\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): query of shape (N, L_t, E_q)\n",
    "            key (torch.Tensor): key of shape (N, L_s, E_k)\n",
    "            value (torch.Tensor): value of shape (N, L_s, E_v)\n",
    "\n",
    "        Returns:\n",
    "            attn_output (torch.Tensor): output of shape (N, L_t, E_q)\n",
    "        \"\"\"\n",
    "        # Step 1. Apply input projection\n",
    "        # TODO: demonstrate packed projection\n",
    "        query = self.query_proj(query)\n",
    "        key = self.key_proj(key)\n",
    "        value = self.value_proj(value)\n",
    "\n",
    "        # Step 2. Split heads and prepare for SDPA\n",
    "        # reshape query, key, value to separate by head\n",
    "        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n",
    "        query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "\n",
    "        # Step 3. Run SDPA\n",
    "        # (N, nheads, L_t, E_head)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query, key, value, is_causal=True)\n",
    "        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
    "\n",
    "        # Step 4. Apply output projection\n",
    "        # (N, L_t, E_total) -> (N, L_t, E_out)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fac66469-c9e3-48e6-a776-1ca13d71dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 512\n",
    "E_q, E_k, E_v, E_total = 512, 512, 512, 512\n",
    "E_out = E_q\n",
    "nheads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e6104bb2-48ca-45f4-b011-921efe5ff52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_sentence_lengths(alpha: float, batch_size: int) -> torch.Tensor:\n",
    "    # generate fake corpus by unigram Zipf distribution\n",
    "    # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858\n",
    "    sentence_lengths = np.empty(batch_size, dtype=int)\n",
    "    for ibatch in range(batch_size):\n",
    "        sentence_lengths[ibatch] = 1\n",
    "        word = np.random.zipf(alpha)\n",
    "        while word != 3 and word != 386 and word != 858:\n",
    "            sentence_lengths[ibatch] += 1\n",
    "            word = np.random.zipf(alpha)\n",
    "    return torch.tensor(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "40bca0e3-cc7a-4a04-916e-3644b7aeb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(N, E_q, E_k, E_v, device):\n",
    "    # generate semi-realistic data using Zipf distribution for sentence lengths\n",
    "    sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N)\n",
    "\n",
    "    # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged\n",
    "    # dimension and works with torch.compile. The batch items each have shape (B, S*, D)\n",
    "    # where B = batch size, S* = ragged sequence length, and D = embedding dimension.\n",
    "    query = torch.nested.nested_tensor([\n",
    "        torch.randn(l.item(), E_q, device=device)\n",
    "        for l in sentence_lengths\n",
    "    ], layout=torch.jagged).to(device=device)\n",
    "\n",
    "    key = torch.nested.nested_tensor([\n",
    "        torch.randn(s.item(), E_k, device=device)\n",
    "        for s in sentence_lengths\n",
    "    ], layout=torch.jagged).to(device=device)\n",
    "\n",
    "    value = torch.nested.nested_tensor([\n",
    "        torch.randn(s.item(), E_v, device=device)\n",
    "        for s in sentence_lengths\n",
    "    ], layout=torch.jagged).to(device=device)\n",
    "\n",
    "    return query, key, value, sentence_lengths\n",
    "\n",
    "query, key, value, sentence_lengths = gen_batch(N, E_q, E_k, E_v, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d818df1e-d72f-4858-a793-6c0bff3437bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jagged_to_padded(jt, padding_val):\n",
    "    # TODO: do jagged -> padded directly when this is supported\n",
    "    return torch.nested.to_padded_tensor(\n",
    "        torch.nested.nested_tensor(list(jt.unbind())),\n",
    "        padding_val).to(device=device)\n",
    "\n",
    "padded_query, padded_key, padded_value = (\n",
    "    jagged_to_padded(t, 0.0) for t in (query, key, value)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71664dc7-d949-4a47-93f7-b2f0a21f6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(E_q, E_k, E_v, E_total, nheads).to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07b25de-fa5d-493e-8a6a-b24a3e29bd0c",
   "metadata": {},
   "source": [
    "## Timing MHA Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b7783769-77ad-48b4-b909-7e7205c50719",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 590.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ragged tensor runtime: 0.0016675735265016555 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:01<00:00, 98.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average padded tensor runtime: 0.010072691394016146 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark(func, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    begin = timeit.default_timer()\n",
    "    output = func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    end = timeit.default_timer()\n",
    "    return output, (end - begin)\n",
    "\n",
    "num_trials = 100\n",
    "\n",
    "compiled_time_nested = []\n",
    "compiled_time_padded = []\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha = torch.compile(\n",
    "            mha, \n",
    "            #options={\"triton.cudagraphs\": True}, \n",
    "            fullgraph=True,\n",
    "            backend='cudagraphs'\n",
    "    )\n",
    "\n",
    "compiled_mha(query, key, value)\n",
    "\n",
    "# ...now benchmark\n",
    "for _ in tqdm.trange(1, num_trials + 1):\n",
    "    _, t = benchmark(compiled_mha, query, key, value)\n",
    "    compiled_time_nested.append(t)\n",
    "\n",
    "print(\"Average ragged tensor runtime:\", sum(compiled_time_nested) / len(compiled_time_nested), \"seconds\")\n",
    "\n",
    "torch.compiler.reset()\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha = torch.compile(\n",
    "            mha, \n",
    "            #options={\"triton.cudagraphs\": True}, \n",
    "            fullgraph=True,\n",
    "            backend='cudagraphs'\n",
    "    )\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha(padded_query, padded_key, padded_value)\n",
    "# ...now benchmark\n",
    "\n",
    "for _ in tqdm.trange(1, num_trials + 1):\n",
    "    _, t = benchmark(compiled_mha, padded_query, padded_key, padded_value)\n",
    "    compiled_time_padded.append(t)\n",
    "\n",
    "print(\"Average padded tensor runtime:\", sum(compiled_time_padded) / len(compiled_time_padded), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24d8dd1-85a9-41e6-8c4d-b80b912d1c2c",
   "metadata": {},
   "source": [
    "## Timing MHA JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "451952b1-96d3-4ea7-ad44-c85c899da397",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                                                                    | 0/10 [00:00<?, ?it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 10%|██████████████                                                                                                                              | 1/10 [00:00<00:02,  3.10it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 20%|████████████████████████████                                                                                                                | 2/10 [00:00<00:02,  3.56it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 30%|██████████████████████████████████████████                                                                                                  | 3/10 [00:00<00:01,  3.74it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 40%|████████████████████████████████████████████████████████                                                                                    | 4/10 [00:01<00:01,  3.83it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 50%|██████████████████████████████████████████████████████████████████████                                                                      | 5/10 [00:01<00:01,  3.89it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 60%|████████████████████████████████████████████████████████████████████████████████████                                                        | 6/10 [00:01<00:01,  3.92it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 70%|██████████████████████████████████████████████████████████████████████████████████████████████████                                          | 7/10 [00:01<00:00,  3.95it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 80%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████                            | 8/10 [00:02<00:00,  3.63it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      " 90%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 9/10 [00:02<00:00,  3.74it/s]skipping cudagraphs due to skipping cudagraphs due to cpu device (zeros)\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:02<00:00,  3.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average ragged tensor JIT time: 0.26472784793004395 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average padded tensor JIT time: 0.15139416206628084 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark_jit(func, *args, **kwargs):\n",
    "    torch.compiler.reset()\n",
    "    torch.cuda.synchronize()\n",
    "    begin = timeit.default_timer()\n",
    "    compiled_func = torch.compile(\n",
    "                func, \n",
    "                #options={\"triton.cudagraphs\": True}, \n",
    "                fullgraph=True,\n",
    "                backend='cudagraphs'\n",
    "        )\n",
    "    output = compiled_func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    end = timeit.default_timer()\n",
    "    return output, (end - begin)\n",
    "\n",
    "num_jit_trials = 10\n",
    "\n",
    "jit_time_nested = []\n",
    "jit_time_padded = []\n",
    "\n",
    "for _ in tqdm.trange(1, num_jit_trials + 1):\n",
    "    _, t = benchmark_jit(mha, query, key, value)\n",
    "    jit_time_nested.append(t)\n",
    "\n",
    "print(\"Average ragged tensor JIT time:\", sum(jit_time_nested) / len(jit_time_nested), \"seconds\")\n",
    "\n",
    "for _ in tqdm.trange(1, num_trials + 1):\n",
    "    _, t = benchmark_jit(mha, padded_query, padded_key, padded_value)\n",
    "    jit_time_padded.append(t)\n",
    "\n",
    "print(\"Average padded tensor JIT time:\", sum(jit_time_padded) / len(jit_time_padded), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0747a41a-0bc2-4351-a670-0f0f6f802721",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
