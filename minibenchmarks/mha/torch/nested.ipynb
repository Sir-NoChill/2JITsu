{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f87cb309-8904-4b6d-9ffe-1a2b5be6f483",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn\n",
    "\n",
    "torch.manual_seed(1)\n",
    "np.random.seed(1)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "302c44cb-a27d-45de-95cc-3051c4bb7523",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes multi-head attention. Supports nested or padded tensors.\n",
    "\n",
    "    Args:\n",
    "        E_q (int): Size of embedding dim for query\n",
    "        E_k (int): Size of embedding dim for key\n",
    "        E_v (int): Size of embedding dim for value\n",
    "        E_total (int): Total embedding dim of combined heads post input projection. Each head\n",
    "            has dim E_total // nheads\n",
    "        nheads (int): Number of heads\n",
    "        dropout_p (float, optional): Dropout probability. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, E_q: int, E_k: int, E_v: int, E_total: int,\n",
    "                 nheads: int, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.query_proj = nn.Linear(E_q, E_total)\n",
    "        self.key_proj = nn.Linear(E_k, E_total)\n",
    "        self.value_proj = nn.Linear(E_v, E_total)\n",
    "        E_out = E_q\n",
    "        self.out_proj = nn.Linear(E_total, E_out)\n",
    "        assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.E_head = E_total // nheads\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass; runs the following process:\n",
    "            1. Apply input projection\n",
    "            2. Split heads and prepare for SDPA\n",
    "            3. Run SDPA\n",
    "            4. Apply output projection\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): query of shape (N, L_t, E_q)\n",
    "            key (torch.Tensor): key of shape (N, L_s, E_k)\n",
    "            value (torch.Tensor): value of shape (N, L_s, E_v)\n",
    "\n",
    "        Returns:\n",
    "            attn_output (torch.Tensor): output of shape (N, L_t, E_q)\n",
    "        \"\"\"\n",
    "        # Step 1. Apply input projection\n",
    "        # TODO: demonstrate packed projection\n",
    "        query = self.query_proj(query)\n",
    "        key = self.key_proj(key)\n",
    "        value = self.value_proj(value)\n",
    "\n",
    "        # Step 2. Split heads and prepare for SDPA\n",
    "        # reshape query, key, value to separate by head\n",
    "        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n",
    "        query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "\n",
    "        # Step 3. Run SDPA\n",
    "        # (N, nheads, L_t, E_head)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query, key, value, dropout_p=dropout_p, is_causal=True)\n",
    "        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
    "\n",
    "        # Step 4. Apply output projection\n",
    "        # (N, L_t, E_total) -> (N, L_t, E_out)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3176b7f2-3d53-4f67-b64d-8a310cf2e5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention2(nn.Module):\n",
    "    \"\"\"\n",
    "    Computes multi-head attention. Supports nested or padded tensors.\n",
    "\n",
    "    Args:\n",
    "        E_q (int): Size of embedding dim for query\n",
    "        E_k (int): Size of embedding dim for key\n",
    "        E_v (int): Size of embedding dim for value\n",
    "        E_total (int): Total embedding dim of combined heads post input projection. Each head\n",
    "            has dim E_total // nheads\n",
    "        nheads (int): Number of heads\n",
    "        dropout_p (float, optional): Dropout probability. Default: 0.0\n",
    "    \"\"\"\n",
    "    def __init__(self, E_q: int, E_k: int, E_v: int, E_total: int,\n",
    "                 nheads: int, dropout_p: float = 0.0):\n",
    "        super().__init__()\n",
    "        self.nheads = nheads\n",
    "        self.dropout_p = dropout_p\n",
    "        self.query_proj = nn.Linear(E_q, E_total)\n",
    "        self.key_proj = nn.Linear(E_k, E_total)\n",
    "        self.value_proj = nn.Linear(E_v, E_total)\n",
    "        E_out = E_q\n",
    "        self.out_proj = nn.Linear(E_total, E_out)\n",
    "        assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.E_head = E_total // nheads\n",
    "\n",
    "    def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor, start_inds: torch.Tensor, end_inds: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass; runs the following process:\n",
    "            1. Apply input projection\n",
    "            2. Split heads and prepare for SDPA\n",
    "            3. Run SDPA\n",
    "            4. Apply output projection\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): query of shape (N * ?, E_q)\n",
    "            key (torch.Tensor): key of shape (N * ?, E_k)\n",
    "            value (torch.Tensor): value of shape (N * ?, E_v)\n",
    "            slice_inds (torch.Tensor): value of shape (N, 2)\n",
    "\n",
    "        Returns:\n",
    "            attn_output (torch.Tensor): output of shape (N, L_t, E_q)\n",
    "        \"\"\"\n",
    "        # Step 1. Apply input projection\n",
    "        # TODO: demonstrate packed projection\n",
    "        q_f = lambda s, e: self.query_proj(query[s:e, :])\n",
    "        q_f = torch.vmap(q_f)\n",
    "        q_f(start_inds, end_inds)\n",
    "        \n",
    "\n",
    "        return\n",
    "        \n",
    "        # query = self.query_proj(query)\n",
    "        key = self.key_proj(key)\n",
    "        value = self.value_proj(value)\n",
    "\n",
    "        # Step 2. Split heads and prepare for SDPA\n",
    "        # reshape query, key, value to separate by head\n",
    "        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n",
    "        query = query.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        key = key.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        value = value.unflatten(-1, [self.nheads, self.E_head]).transpose(1, 2)\n",
    "\n",
    "        # Step 3. Run SDPA\n",
    "        # (N, nheads, L_t, E_head)\n",
    "        attn_output = F.scaled_dot_product_attention(\n",
    "            query, key, value, dropout_p=dropout_p, is_causal=True)\n",
    "        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n",
    "        attn_output = attn_output.transpose(1, 2).flatten(-2)\n",
    "\n",
    "        # Step 4. Apply output projection\n",
    "        # (N, L_t, E_total) -> (N, L_t, E_out)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fac66469-c9e3-48e6-a776-1ca13d71dfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 512\n",
    "E_q, E_k, E_v, E_total = 512, 512, 512, 512\n",
    "E_out = E_q\n",
    "nheads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6aa16f94-f66c-4f2e-a637-c5464bd4ee82",
   "metadata": {},
   "outputs": [],
   "source": [
    "dropout_p = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e6104bb2-48ca-45f4-b011-921efe5ff52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_sentence_lengths(alpha: float, batch_size: int) -> torch.Tensor:\n",
    "    # generate fake corpus by unigram Zipf distribution\n",
    "    # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858\n",
    "    sentence_lengths = np.empty(batch_size, dtype=int)\n",
    "    for ibatch in range(batch_size):\n",
    "        sentence_lengths[ibatch] = 1\n",
    "        word = np.random.zipf(alpha)\n",
    "        while word != 3 and word != 386 and word != 858:\n",
    "            sentence_lengths[ibatch] += 1\n",
    "            word = np.random.zipf(alpha)\n",
    "    return torch.tensor(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40bca0e3-cc7a-4a04-916e-3644b7aeb629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(N, E_q, E_k, E_v, device):\n",
    "    # generate semi-realistic data using Zipf distribution for sentence lengths\n",
    "    sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N)\n",
    "\n",
    "    # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged\n",
    "    # dimension and works with torch.compile. The batch items each have shape (B, S*, D)\n",
    "    # where B = batch size, S* = ragged sequence length, and D = embedding dimension.\n",
    "    query = torch.cat([\n",
    "        torch.randn(l.item(), E_q, device=device)\n",
    "        for l in sentence_lengths\n",
    "    ])\n",
    "\n",
    "    key = torch.cat([\n",
    "        torch.randn(s.item(), E_k, device=device)\n",
    "        for s in sentence_lengths\n",
    "    ])\n",
    "\n",
    "    value = torch.cat([\n",
    "        torch.randn(s.item(), E_v, device=device)\n",
    "        for s in sentence_lengths\n",
    "    ])\n",
    "\n",
    "    inds = [0]\n",
    "    for s in sentence_lengths:\n",
    "        inds.append(inds[-1] + s.item())\n",
    "\n",
    "    slice_inds = torch.stack([torch.tensor(inds[:-1]), torch.tensor(inds[1:])], 1)\n",
    "    \n",
    "    return query, key, value, slice_inds, sentence_lengths\n",
    "\n",
    "query, key, value, slice_inds, sentence_lengths = gen_batch(N, E_q, E_k, E_v, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d818df1e-d72f-4858-a793-6c0bff3437bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jagged_to_padded(jt, padding_val):\n",
    "    # TODO: do jagged -> padded directly when this is supported\n",
    "    return torch.nested.to_padded_tensor(\n",
    "        torch.nested.nested_tensor(list(jt.unbind())),\n",
    "        padding_val)\n",
    "\n",
    "padded_query, padded_key, padded_value = (\n",
    "    jagged_to_padded(t, 0.0) for t in (query, key, value)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71664dc7-d949-4a47-93f7-b2f0a21f6105",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention2(E_q, E_k, E_v, E_total, nheads, dropout_p).to(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "448bfabd-9cb5-4ecf-abfd-f266dbd78fa5",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_inds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mslice_inds\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/2JITsu/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/2JITsu/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[14], line 48\u001b[0m, in \u001b[0;36mMultiHeadAttention2.forward\u001b[0;34m(self, query, key, value, start_inds, end_inds)\u001b[0m\n\u001b[1;32m     46\u001b[0m q_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s, e: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_proj(query[s:e, :])\n\u001b[1;32m     47\u001b[0m q_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvmap(q_f)\n\u001b[0;32m---> 48\u001b[0m \u001b[43mq_f\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_inds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_inds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# query = self.query_proj(query)\u001b[39;00m\n",
      "File \u001b[0;32m~/mambaforge/envs/2JITsu/lib/python3.11/site-packages/torch/_functorch/apis.py:203\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/2JITsu/lib/python3.11/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/mambaforge/envs/2JITsu/lib/python3.11/site-packages/torch/_functorch/vmap.py:479\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    476\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    477\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    478\u001b[0m     )\n\u001b[0;32m--> 479\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "Cell \u001b[0;32mIn[14], line 46\u001b[0m, in \u001b[0;36mMultiHeadAttention2.forward.<locals>.<lambda>\u001b[0;34m(s, e)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;124;03mForward pass; runs the following process:\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    1. Apply input projection\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    attn_output (torch.Tensor): output of shape (N, L_t, E_q)\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Step 1. Apply input projection\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# TODO: demonstrate packed projection\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m q_f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m s, e: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquery_proj(\u001b[43mquery\u001b[49m\u001b[43m[\u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     47\u001b[0m q_f \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvmap(q_f)\n\u001b[1;32m     48\u001b[0m q_f(start_inds, end_inds)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: vmap: It looks like you're calling .item() on a Tensor. We don't support vmap over calling .item() on a Tensor, please try to rewrite what you're doing with other operations. If error is occurring somewhere inside PyTorch internals, please file a bug report."
     ]
    }
   ],
   "source": [
    "mha(query, key, value, slice_inds[:, 0], slice_inds[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7783769-77ad-48b4-b909-7e7205c50719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512, j1, 512])\n",
      "torch.Size([512, j1, 512])\n",
      "torch.Size([512, 128, 512])\n",
      "torch.Size([512, 128, 512])\n",
      "=== without torch.compile ===\n",
      "nested and padded calculations differ by 0.0\n",
      "nested tensor multi-head attention takes 0.01481871772557497 seconds\n",
      "padded tensor multi-head attention takes 0.005708473734557629 seconds\n",
      "torch.Size([512, j1, 512])\n",
      "torch.Size([512, j1, 512])\n",
      "torch.Size([512, j1, 512])\n",
      "torch.Size([512, j1, 512])\n",
      "torch.Size([512, 128, 512])\n",
      "torch.Size([512, 128, 512])\n",
      "torch.Size([512, 128, 512])\n",
      "torch.Size([512, 128, 512])\n",
      "=== with torch.compile ===\n",
      "nested and padded calculations differ by 0.0\n",
      "nested tensor multi-head attention takes 0.002335646189749241 seconds\n",
      "padded tensor multi-head attention takes 0.005724301561713219 seconds\n"
     ]
    }
   ],
   "source": [
    "def benchmark(func, *args, **kwargs):\n",
    "    torch.cuda.synchronize()\n",
    "    begin = timeit.default_timer()\n",
    "    output = func(*args, **kwargs)\n",
    "    torch.cuda.synchronize()\n",
    "    end = timeit.default_timer()\n",
    "    return output, (end - begin)\n",
    "\n",
    "output_nested, time_nested = benchmark(mha, query, key, value)\n",
    "output_padded, time_padded = benchmark(mha, padded_query, padded_key, padded_value)\n",
    "\n",
    "# padding-specific step: remove output projection bias from padded entries for fair comparison\n",
    "for i, entry_length in enumerate(sentence_lengths):\n",
    "    output_padded[i, entry_length:] = 0.0\n",
    "\n",
    "print(\"=== without torch.compile ===\")\n",
    "print(\"nested and padded calculations differ by\", (jagged_to_padded(output_nested, 0.0) - output_padded).abs().max().item())\n",
    "print(\"nested tensor multi-head attention takes\", time_nested, \"seconds\")\n",
    "print(\"padded tensor multi-head attention takes\", time_padded, \"seconds\")\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha = torch.compile(mha)\n",
    "compiled_mha(query, key, value)\n",
    "# ...now benchmark\n",
    "compiled_output_nested, compiled_time_nested = benchmark(\n",
    "    compiled_mha, query, key, value)\n",
    "\n",
    "# warm up compile first...\n",
    "compiled_mha(padded_query, padded_key, padded_value)\n",
    "# ...now benchmark\n",
    "compiled_output_padded, compiled_time_padded = benchmark(\n",
    "    compiled_mha, padded_query, padded_key, padded_value)\n",
    "\n",
    "# padding-specific step: remove output projection bias from padded entries for fair comparison\n",
    "for i, entry_length in enumerate(sentence_lengths):\n",
    "    compiled_output_padded[i, entry_length:] = 0.0\n",
    "\n",
    "print(\"=== with torch.compile ===\")\n",
    "print(\"nested and padded calculations differ by\", (jagged_to_padded(compiled_output_nested, 0.0) - compiled_output_padded).abs().max().item())\n",
    "print(\"nested tensor multi-head attention takes\", compiled_time_nested, \"seconds\")\n",
    "print(\"padded tensor multi-head attention takes\", compiled_time_padded, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9c55778a-2ee3-4764-bc8a-b9318c374962",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.5005,  3.0300, -5.8522])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batched_dot = torch.func.vmap(torch.dot)  # [N, D], [N, D] -> [N]\n",
    "x, y = torch.randn(3, 5), torch.randn(3, 5)\n",
    "batched_dot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c35e783f-db2e-4a33-875e-63c2003f774f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
