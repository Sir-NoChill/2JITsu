{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b17710a6-a5fb-4a12-9ba4-16e19a68e090",
   "metadata": {},
   "source": [
    "# Multi-Head Attention Minibenchmark\n",
    "This benchmark is adapted from [this Pytorch tutorial](https://pytorch.org/tutorials/prototype/nestedtensor.html).\n",
    "\n",
    "This benchmark expects packages from the `requirements.txt` in the root directory to be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6df741a0-affd-4ff0-aa81-a50f028dd49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import timeit\n",
    "import jax\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from jax import numpy as jnp\n",
    "from flax import nnx\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "beb3638a-3b92-4040-9fb1-afeb80de5c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):  #@save\n",
    "    E_q: int\n",
    "    E_k: int\n",
    "    E_v: int    \n",
    "    E_total: int\n",
    "    nheads: int\n",
    "    \n",
    "    def setup(self):\n",
    "        rngs = nnx.Rngs(0)\n",
    "\n",
    "        self.query_proj = nnx.Linear(E_q, E_total, rngs=rngs)\n",
    "        self.key_proj = nnx.Linear(E_k, E_total, rngs=rngs)\n",
    "        self.value_proj = nnx.Linear(E_v, E_total, rngs=rngs)\n",
    "        E_out = self.E_q\n",
    "        self.out_proj = nnx.Linear(E_total, E_out, rngs=rngs)\n",
    "        assert E_total % nheads == 0, \"Embedding dim is not divisible by nheads\"\n",
    "        self.E_head = E_total // nheads\n",
    "        self.E_q_last = self.E_q - 1\n",
    "        self.E_k_last = self.E_k - 1\n",
    "        self.E_v_last = self.E_v - 1        \n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, queries, keys, values):\n",
    "        query = self.query_proj(queries)\n",
    "        key = self.key_proj(keys)\n",
    "        value = self.value_proj(values)\n",
    "\n",
    "        # Step 2. Split heads and prepare for SDPA\n",
    "        # reshape query, key, value to separate by head\n",
    "        # (N, L_t, E_total) -> (N, L_t, nheads, E_head) -> (N, nheads, L_t, E_head)\n",
    "        query = jnp.reshape(query, (query.shape[0], query.shape[1], self.nheads, self.E_head)).transpose(0, 2, 1, 3)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        key = jnp.reshape(key, (key.shape[0], key.shape[1], self.nheads, self.E_head)).transpose(0, 2, 1, 3)\n",
    "        # (N, L_s, E_total) -> (N, L_s, nheads, E_head) -> (N, nheads, L_s, E_head)\n",
    "        value = jnp.reshape(value, (value.shape[0], value.shape[1], self.nheads, self.E_head)).transpose(0, 2, 1, 3)\n",
    "\n",
    "        # Step 3. Run SDPA\n",
    "        # (N, nheads, L_t, E_head)\n",
    "        attn_output = jax.nn.dot_product_attention(query, key, value, is_causal=True)\n",
    "        \n",
    "        # (N, nheads, L_t, E_head) -> (N, L_t, nheads, E_head) -> (N, L_t, E_total)\n",
    "        attn_output = jnp.transpose(attn_output, (0, 2, 1, 3))\n",
    "        attn_output = attn_output.reshape(attn_output.shape[0], attn_output.shape[1], E_total)\n",
    "\n",
    "        # Step 4. Apply output projection\n",
    "        # (N, L_t, E_total) -> (N, L_t, E_out)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "\n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d9acd5ce-ddd6-4274-bb64-cf29fe6530b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rkey = jax.random.key(1)\n",
    "N = 512\n",
    "E_q, E_k, E_v, E_total = 512, 512, 512, 512\n",
    "E_out = E_q\n",
    "nheads = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad27b7b0-d8ad-4f60-a94f-9c75492e4840",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zipf_sentence_lengths(alpha: float, batch_size: int):\n",
    "    # generate fake corpus by unigram Zipf distribution\n",
    "    # from wikitext-2 corpus, we get rank \".\" = 3, \"!\" = 386, \"?\" = 858\n",
    "    sentence_lengths = np.empty(batch_size, dtype=int)\n",
    "    for ibatch in range(batch_size):\n",
    "        sentence_lengths[ibatch] = 1\n",
    "        word = np.random.zipf(alpha)\n",
    "        while word != 3 and word != 386 and word != 858:\n",
    "            sentence_lengths[ibatch] += 1\n",
    "            word = np.random.zipf(alpha)\n",
    "    return jnp.asarray(sentence_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4bdc3dd3-29a9-480f-b5cd-aff3265d5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_batch(N, E_q, E_k, E_v):\n",
    "    # generate semi-realistic data using Zipf distribution for sentence lengths\n",
    "    sentence_lengths = zipf_sentence_lengths(alpha=1.2, batch_size=N)\n",
    "\n",
    "    # Note: the torch.jagged layout is a nested tensor layout that supports a single ragged\n",
    "    # dimension and works with torch.compile. The batch items each have shape (B, S*, D)\n",
    "    # where B = batch size, S* = ragged sequence length, and D = embedding dimension.\n",
    "    max_l = max(sentence_lengths)\n",
    "    query = jnp.stack([\n",
    "        jnp.pad(jax.random.uniform(rkey, (l.item(), E_q)), ((0, max_l - l.item()), (0, 0)))\n",
    "        for l in sentence_lengths\n",
    "    ])\n",
    "\n",
    "    key = jnp.stack([\n",
    "        jnp.pad(jax.random.uniform(rkey, (l.item(), E_k)), ((0, max_l - l.item()), (0, 0)))\n",
    "        for l in sentence_lengths\n",
    "    ])\n",
    "\n",
    "    value = jnp.stack([\n",
    "        jnp.pad(jax.random.uniform(rkey, (l.item(), E_v)), ((0, max_l - l.item()), (0, 0)))\n",
    "        for l in sentence_lengths\n",
    "    ])\n",
    "\n",
    "    return query, key, value, sentence_lengths\n",
    "\n",
    "query, key, value, sentence_lengths = gen_batch(N, E_q, E_k, E_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "48e13b65-87c9-4fb0-b041-45790136302e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mha = MultiHeadAttention(E_q, E_k, E_v, E_total, nheads)\n",
    "params = mha.init(rkey, query, key, value)\n",
    "# mha.apply(params, query, key, value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a41d09-a34b-4161-8bdf-164edfbdab60",
   "metadata": {},
   "source": [
    "## Timing MHA Runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd592529-7239-4bb0-b1ee-b7235894a06c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 550.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average padded tensor runtime: 0.0018002113793045282 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark(func, params, query, key, value):\n",
    "    begin = timeit.default_timer()\n",
    "    output = func(params, query, key, value)\n",
    "    end = timeit.default_timer()\n",
    "    return output, (end - begin)\n",
    "\n",
    "num_trials = 100\n",
    "\n",
    "# warmup\n",
    "jit_model = jax.jit(mha.apply, backend='gpu').lower(params, query, key, value)\n",
    "compiled_model = jit_model.compile()\n",
    "compiled_model(params, query, key, value)\n",
    "\n",
    "padded_time = []\n",
    "\n",
    "for _ in tqdm.trange(1, num_trials + 1):\n",
    "    _, t = benchmark(compiled_model, params, query, key, value)\n",
    "    padded_time.append(t)\n",
    "print(\"Average padded tensor runtime:\", sum(padded_time) / len(padded_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20dba0bd-9418-4023-8bc4-7afd8d721eb3",
   "metadata": {},
   "source": [
    "## Timing MHA JIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "767529d5-e368-4d67-983f-70d027902c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:03<00:00,  3.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average padded tensor JIT time: 0.3156730689108372 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def benchmark_jit(func, params, query, key, value):\n",
    "    jax.clear_caches()\n",
    "    begin = timeit.default_timer()\n",
    "    jit_model = jax.jit(func.apply, backend='gpu').lower(params, query, key, value)\n",
    "    compiled_model = jit_model.compile()\n",
    "    end = timeit.default_timer()\n",
    "    return output, (end - begin)\n",
    "\n",
    "num_trials = 10\n",
    "\n",
    "# warmup\n",
    "for _ in range(5):\n",
    "    benchmark_jit(mha, params, query, key, value)\n",
    "    \n",
    "jit_time = []\n",
    "\n",
    "for _ in tqdm.trange(1, num_trials + 1):\n",
    "    _, t = benchmark_jit(mha, params, query, key, value)\n",
    "    jit_time.append(t)\n",
    "print(\"Average padded tensor JIT time:\", sum(jit_time) / len(jit_time), \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1638466-fbb5-479c-a6ae-56462a630dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
