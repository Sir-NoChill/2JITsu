{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a2a35b-8029-4789-ac4c-e7de06ad563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'dlrm' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/dlrm.git\n",
    "!touch ./dlrm/__ini__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f548db3-2723-4bbe-b72c-c12b7684c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment params\n",
    "num_train_trials = 5\n",
    "num_train_warmups = 1\n",
    "num_jit_trials = 10\n",
    "num_jit_warmups = 2\n",
    "num_inference_trials = 10000\n",
    "num_inference_warmups = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e594d43-7767-452e-8331-10e0a05629fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages', '/home/achilibe/Code/2JITsu/ViT/torch/vit_pytorch', '/home/achilibe/Code/2JITsu/BERT/torch/benchmark/torchbenchmark/models/BERT_pytorch', '/home/achilibe/Code/2JITsu/DLRM/torch/dlrm']\n",
      "Unable to import mlperf_logging,  No module named 'mlperf_logging'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-22 14:15:24.357723: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-22 14:15:24.365017: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-22 14:15:24.372967: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-22 14:15:24.375364: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-22 14:15:24.381914: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-22 14:15:24.775215: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd()/Path(\"dlrm\")))\n",
    "print(sys.path)\n",
    "\n",
    "from dlrm.dlrm_s_pytorch import *\n",
    "\n",
    "class Args:\n",
    "    arch_sparse_feature_size=16\n",
    "    arch_embedding_size='4-3-2'\n",
    "    arch_mlp_bot='13-128-64-32'\n",
    "    arch_mlp_top='80-128-64-1'\n",
    "    arch_interaction_op='dot'\n",
    "    arch_interaction_itself=False\n",
    "    weighted_pooling=None\n",
    "    loss_function='mse'\n",
    "    md_flag=False\n",
    "    md_threshold=200\n",
    "    md_temperature=0.3\n",
    "    md_round_dims=False\n",
    "    qr_flag=False\n",
    "    qr_threshold=200\n",
    "    qr_operation='mult'\n",
    "    qr_collisions=4\n",
    "    activation_function='relu'\n",
    "    loss_function='bce'\n",
    "    loss_weights='1.0-1.0'\n",
    "    loss_threshold=0.0\n",
    "    round_targets=True\n",
    "    data_size=1\n",
    "    num_batches=0\n",
    "    data_generation='random'\n",
    "    rand_data_dist='uniform'\n",
    "    rand_data_min=0\n",
    "    rand_data_max=1\n",
    "    rand_data_mu=-1\n",
    "    rand_data_sigma=1\n",
    "    data_trace_file='./input/dist_emb_j.log'\n",
    "    data_set='kaggle'\n",
    "    raw_data_file='./data_kaggle/train.txt'\n",
    "    processed_data_file='./data_kaggle/kaggleAdDisplayChallenge_processed.npz'\n",
    "    data_randomize='total'\n",
    "    data_trace_enable_padding=False\n",
    "    max_ind_range=-1\n",
    "    data_sub_sample_rate=0.0\n",
    "    num_indices_per_lookup=10\n",
    "    num_indices_per_lookup_fixed=False\n",
    "    num_workers=0\n",
    "    memory_map=False\n",
    "    mini_batch_size=8192\n",
    "    nepochs=1\n",
    "    learning_rate=0.1\n",
    "    print_precision=5\n",
    "    numpy_rand_seed=123\n",
    "    sync_dense_params=True\n",
    "    optimizer='sgd'\n",
    "    dataset_multiprocessing=True\n",
    "    inference_only=False\n",
    "    quantize_mlp_with_bit=32\n",
    "    quantize_emb_with_bit=32\n",
    "    save_onnx=False\n",
    "    use_gpu=True\n",
    "    local_rank=-1\n",
    "    dist_backend=''\n",
    "    print_freq=1024\n",
    "    test_freq=1024\n",
    "    test_mini_batch_size=8192\n",
    "    test_num_workers=-1\n",
    "    print_time=False\n",
    "    print_wall_time=False\n",
    "    debug_mode=True\n",
    "    enable_profiling=False\n",
    "    plot_compute_graph=False\n",
    "    tensor_board_filename='run_kaggle_pt'\n",
    "    save_model=''\n",
    "    load_model=''\n",
    "    mlperf_logging=False\n",
    "    mlperf_acc_threshold=0.0\n",
    "    mlperf_auc_threshold=0.0\n",
    "    mlperf_bin_loader=False\n",
    "    mlperf_bin_shuffle=False\n",
    "    mlperf_grad_accum_iter=1\n",
    "    lr_num_warmup_steps=0\n",
    "    lr_decay_start_step=0\n",
    "    lr_num_decay_steps=0\n",
    "    nbatches=1000\n",
    "\n",
    "global args\n",
    "args = Args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46834e70-bf19-4684-aba2-892c9650edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████| 10/10 [00:00<00:00, 21.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average JIT time: 0.00014212396409776475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "import torch._dynamo.utils\n",
    "from torch._dynamo.eval_frame import _debug_get_cache_entry_list\n",
    "\n",
    "dlrm = DLRM_Net(\n",
    "    16,\n",
    "    np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\"),\n",
    "    np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\"),\n",
    "    np.fromstring(args.arch_mlp_top, dtype=int, sep=\"-\"),\n",
    "    arch_interaction_op='cat',\n",
    "    arch_interaction_itself=False,\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=2\n",
    ")\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['TORCHINDUCTOR_FORCE_DISABLE_CACHES'] = '1'\n",
    "\n",
    "def time_jit(num_jit_runs: int, num_jit_warmups: int, model: nn.Module) -> float:\n",
    "    times = []\n",
    "    for i in tqdm.trange(1, num_jit_runs + 1):\n",
    "        start_jit_time = time.time()\n",
    "        \n",
    "        jit_model = torch.compile(\n",
    "            model, \n",
    "            options={\"triton.cudagraphs\": True}, \n",
    "            fullgraph=True,\n",
    "            backend='cudagraphs'\n",
    "        )\n",
    "\n",
    "        end_jit_time = time.time()\n",
    "        #print(_debug_get_cache_entry_list(jit_model._torchdynamo_orig_callable))\n",
    "        torch.compiler.reset()\n",
    "        torch._dynamo.reset()\n",
    "        torch.cuda.empty_cache()\n",
    "        #print(_debug_get_cache_entry_list(jit_model._torchdynamo_orig_callable))\n",
    "        if i >= num_jit_warmups:\n",
    "            times.append(end_jit_time - start_jit_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "    \n",
    "print(torch._dynamo.list_backends())\n",
    "average_jit_time = time_jit(num_jit_trials, num_jit_warmups, dlrm)\n",
    "print(f\"Average JIT time: {average_jit_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0a901d4b-2e96-4496-b815-af2775e0d7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kagglehub in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (0.3.3)\n",
      "Requirement already satisfied: packaging in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from kagglehub) (24.1)\n",
      "Requirement already satisfied: requests in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from kagglehub) (2.32.3)\n",
      "Requirement already satisfied: tqdm in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from kagglehub) (4.66.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from requests->kagglehub) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from requests->kagglehub) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from requests->kagglehub) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages (from requests->kagglehub) (2024.8.30)\n"
     ]
    }
   ],
   "source": [
    "!pip install kagglehub\n",
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"mrkmakr/criteo-dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "70cfccb5-0cf3-4f0d-93d8-3eab1e98ab78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlrm.dlrm_data_pytorch as dp\n",
    "import numpy as np\n",
    "\n",
    "ln_emb = np.fromstring(args.arch_embedding_size, dtype=int, sep=\"-\")\n",
    "m_den = np.fromstring(args.arch_mlp_bot, dtype=int, sep=\"-\")[0]\n",
    "\n",
    "use_gpu = False\n",
    "global device\n",
    "if use_gpu: \n",
    "    device = torch.device(\"cuda\") \n",
    "else: \n",
    "    device = torch.device(\"cpu\") \n",
    "\n",
    "train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(\n",
    "    args, ln_emb, m_den\n",
    ")\n",
    "\n",
    "parameters = (\n",
    "    dlrm.parameters()\n",
    "    if ext_dist.my_size == 1\n",
    "    else [\n",
    "        {\n",
    "            \"params\": [p for emb in dlrm.emb_l for p in emb.parameters()],\n",
    "            \"lr\": args.learning_rate,\n",
    "        },\n",
    "        # TODO check this lr setup\n",
    "        # bottom mlp has no data parallelism\n",
    "        # need to check how do we deal with top mlp\n",
    "        {\n",
    "            \"params\": dlrm.bot_l.parameters(),\n",
    "            \"lr\": args.learning_rate,\n",
    "        },\n",
    "        {\n",
    "            \"params\": dlrm.top_l.parameters(),\n",
    "            \"lr\": args.learning_rate,\n",
    "        },\n",
    "    ]\n",
    ")\n",
    "optimizer = torch.optim.SGD(parameters, lr=args.learning_rate)\n",
    "lr_scheduler = LRPolicyScheduler(\n",
    "    optimizer,\n",
    "    args.lr_num_warmup_steps,\n",
    "    args.lr_decay_start_step,\n",
    "    args.lr_num_decay_steps,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "320447d8-01af-4b39-8834-d8e160999ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following function is a wrapper to avoid checking this multiple times in th\n",
    "# loop below.\n",
    "def unpack_batch(b):\n",
    "    # Experiment with unweighted samples\n",
    "    return b[0], b[1], b[2], b[3], torch.ones(b[3].size()), None\n",
    "\n",
    "def dlrm_wrap(X, lS_o, lS_i, use_gpu, device, ndevices=1, dlrm=dlrm):\n",
    "    with record_function(\"DLRM forward\"):\n",
    "        if use_gpu:  # .cuda()\n",
    "            # lS_i can be either a list of tensors or a stacked tensor.\n",
    "            # Handle each case below:\n",
    "            if ndevices == 1:\n",
    "                lS_i = (\n",
    "                    [S_i.to(device) for S_i in lS_i]\n",
    "                    if isinstance(lS_i, list)\n",
    "                    else lS_i.to(device)\n",
    "                )\n",
    "                lS_o = (\n",
    "                    [S_o.to(device) for S_o in lS_o]\n",
    "                    if isinstance(lS_o, list)\n",
    "                    else lS_o.to(device)\n",
    "                )\n",
    "                print(\"tensors\")\n",
    "        return dlrm(X.to(device), lS_o, lS_i)\n",
    "\n",
    "def loss_fn_wrap(Z, T, use_gpu, device):\n",
    "    with record_function(\"DLRM loss compute\"):\n",
    "        if args.loss_function == \"mse\" or args.loss_function == \"bce\":\n",
    "            return dlrm.loss_fn(Z, T.to(device))\n",
    "        elif args.loss_function == \"wbce\":\n",
    "            loss_ws_ = dlrm.loss_ws[T.data.view(-1).long()].view_as(T).to(device)\n",
    "            loss_fn_ = dlrm.loss_fn(Z, T.to(device))\n",
    "            loss_sc_ = loss_ws_ * loss_fn_\n",
    "            return loss_sc_.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a602cb43-e388-4d84-b0be-cd1ba24fd749",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "(tensor([[0.6965, 0.2861, 0.2269, 0.5513, 0.7195, 0.4231, 0.9808, 0.6848, 0.4809,\n",
      "         0.3921, 0.3432, 0.7290, 0.4386]]), tensor([[0],\n",
      "        [0],\n",
      "        [0]]), [tensor([1]), tensor([0]), tensor([1])], tensor([[1.]]))\n"
     ]
    }
   ],
   "source": [
    "for i, j in enumerate(train_ld):\n",
    "    print(i)\n",
    "    print(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cf73fa0b-b8eb-4f7a-909c-6ac5597cf9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 5/5 [00:02<00:00,  2.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training time: 0.4381582736968994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def time_train(num_train_runs: int, num_train_warmups: int, args: Args):\n",
    "    train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(\n",
    "        args, ln_emb, m_den\n",
    "    )\n",
    "\n",
    "    parameters = (\n",
    "        dlrm.parameters()\n",
    "        if ext_dist.my_size == 1\n",
    "        else [\n",
    "            {\n",
    "                \"params\": [p for emb in dlrm.emb_l for p in emb.parameters()],\n",
    "                \"lr\": args.learning_rate,\n",
    "            },\n",
    "            # TODO check this lr setup\n",
    "            # bottom mlp has no data parallelism\n",
    "            # need to check how do we deal with top mlp\n",
    "            {\n",
    "                \"params\": dlrm.bot_l.parameters(),\n",
    "                \"lr\": args.learning_rate,\n",
    "            },\n",
    "            {\n",
    "                \"params\": dlrm.top_l.parameters(),\n",
    "                \"lr\": args.learning_rate,\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "    optimizer = torch.optim.SGD(parameters, lr=args.learning_rate)\n",
    "    lr_scheduler = LRPolicyScheduler(\n",
    "        optimizer,\n",
    "        args.lr_num_warmup_steps,\n",
    "        args.lr_decay_start_step,\n",
    "        args.lr_num_decay_steps,\n",
    "    )\n",
    "\n",
    "    X, lS_o, lS_i, T, W, CBPP = unpack_batch(enumerate(train_ld).__next__()[1])\n",
    "    \n",
    "    for i in tqdm.trange(num_train_trials):\n",
    "    \n",
    "        \n",
    "        times = []\n",
    "        start_train_time = time.time()\n",
    "        \n",
    "        for i in range(1000):\n",
    "\n",
    "            # forward pass\n",
    "            Z = dlrm_wrap(\n",
    "                X,\n",
    "                lS_o,\n",
    "                lS_i,\n",
    "                use_gpu,  # use gp u\n",
    "                device,\n",
    "                ndevices=1,\n",
    "            )\n",
    "\n",
    "            # loss\n",
    "            E = loss_fn_wrap(Z, T, use_gpu, device)\n",
    "\n",
    "            # compute loss and accuracy\n",
    "            L = E.detach().cpu().numpy()  # numpy array\n",
    "            E.backward()\n",
    "\n",
    "            # optimizer\n",
    "            optimizer.step()\n",
    "            lr_scheduler.step()\n",
    "\n",
    "        end_train_time = time.time()\n",
    "        if i >= num_train_warmups:\n",
    "            times.append(end_train_time - start_train_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "average_training_time = time_train(num_train_trials, num_train_warmups, \n",
    "                                args)\n",
    "print(f\"Average training time: {average_training_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df609275-e160-43c0-b40d-9e048e84622a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 10000/10000 [00:00<00:00, 10985.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 8.954938252766927e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def time_inference(num_inference_trials, num_inference_warmups):\n",
    "    train_data, train_ld, test_data, test_ld = dp.make_random_data_and_loader(\n",
    "        args, ln_emb, m_den\n",
    "    )\n",
    "    \n",
    "    X, lS_o, lS_i, T, W, CBPP = unpack_batch(enumerate(train_ld).__next__()[1])\n",
    "    \n",
    "    times = []\n",
    "    for i in tqdm.trange(num_inference_trials):\n",
    "        start_inference_time = time.time()\n",
    "\n",
    "        dlrm_wrap(\n",
    "            X,\n",
    "            lS_o,\n",
    "            lS_i,\n",
    "            use_gpu,  # use gp u\n",
    "            device,\n",
    "            ndevices=1\n",
    "        )\n",
    "\n",
    "        end_inference_time = time.time()\n",
    "        if i >= num_inference_warmups:\n",
    "            times.append(end_inference_time - start_inference_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "average_inference_time = time_inference(num_inference_trials, num_inference_warmups)\n",
    "print(f\"Average inference time: {average_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e37437-befe-4464-85d7-134dc497fd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export the python file\n",
    "!jupyter nbconvert --to script DLRM-Torch.ipynb\n",
    "!mv DLRM-Torch.py gen_dlrm_torch.py\n",
    "\n",
    "# write the results to the results.csv file\n",
    "import csv\n",
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"Model\", \"Framework\", \"Evaluation\", \"Trials\", \"Warmups\", \"Time\", \"Notes\"])\n",
    "    csvwriter.writerow([\"DLRM\", \"Torch\", \"train\", \n",
    "                       num_train_trials, \n",
    "                       num_train_warmups, \n",
    "                       average_training_time, \n",
    "                       \"\"])\n",
    "    csvwriter.writerow([\"DLRM\", \"Torch\", \"inference\", \n",
    "                       num_inference_trials, \n",
    "                       num_inference_warmups, \n",
    "                       average_inference_time, \n",
    "                       \"\"])\n",
    "    csvwriter.writerow([\"DLRM\", \"Torch\", \"JIT\",\n",
    "                        num_jit_trials,\n",
    "                        num_jit_warmups,\n",
    "                        average_jit_time,\n",
    "                        \"\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
