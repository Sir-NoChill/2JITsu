{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0a2a35b-8029-4789-ac4c-e7de06ad563c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'dlrm' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/facebookresearch/dlrm.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f548db3-2723-4bbe-b72c-c12b7684c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment params\n",
    "num_train_trials = 5\n",
    "num_train_warmups = 1\n",
    "num_jit_trials = 10\n",
    "num_jit_warmups = 2\n",
    "num_inference_trials = 10000\n",
    "num_inference_warmups = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2e594d43-7767-452e-8331-10e0a05629fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/usr/lib/python310.zip', '/usr/lib/python3.10', '/usr/lib/python3.10/lib-dynload', '', '/home/achilibe/Code/2JITsu/venv/lib/python3.10/site-packages', '/home/achilibe/Code/2JITsu/ViT/torch/vit_pytorch', '/home/achilibe/Code/2JITsu/BERT/torch/benchmark/torchbenchmark/models/BERT_pytorch', '/home/achilibe/Code/2JITsu/DLRM/torch/dlrm']\n",
      "Unable to import mlperf_logging,  No module named 'mlperf_logging'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-18 13:22:52.704851: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-10-18 13:22:52.712210: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-10-18 13:22:52.720291: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-10-18 13:22:52.722723: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-10-18 13:22:52.729327: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-18 13:22:53.128728: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "sys.path.append(str(Path.cwd()/Path(\"dlrm\")))\n",
    "print(sys.path)\n",
    "\n",
    "from dlrm.dlrm_s_pytorch import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46834e70-bf19-4684-aba2-892c9650edbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cudagraphs', 'inductor', 'onnxrt', 'openxla', 'tvm']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 21.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average JIT time: 0.00011865297953287761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import torch\n",
    "\n",
    "dlrm = DLRM_Net(\n",
    "    16,\n",
    "    np.array([4,3,2]),\n",
    "    np.array([13,512,256,64,16]),\n",
    "    np.array([512,256,1]),\n",
    "    arch_interaction_op='cat',\n",
    "    arch_interaction_itself=False,\n",
    "    sigmoid_bot=-1,\n",
    "    sigmoid_top=2\n",
    ")\n",
    "\n",
    "def time_jit(num_jit_runs: int, num_jit_warmups: int, model: nn.Module) -> float:\n",
    "    times = []\n",
    "    for i in tqdm.trange(1, num_jit_runs + 1):\n",
    "        start_jit_time = time.time()\n",
    "        \n",
    "        jit_model = torch.compile(\n",
    "            model, \n",
    "            options={\"triton.cudagraphs\": True}, \n",
    "            fullgraph=True,\n",
    "            backend='cudagraphs'\n",
    "        )\n",
    "\n",
    "        end_jit_time = time.time()\n",
    "        torch.compiler.reset()\n",
    "        if i >= num_jit_warmups:\n",
    "            times.append(end_jit_time - start_jit_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "    \n",
    "print(torch._dynamo.list_backends())\n",
    "average_jit_time = time_jit(num_jit_trials, num_jit_warmups, dlrm)\n",
    "print(f\"Average JIT time: {average_jit_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1caf290c-6658-48cb-a42d-55e1ea41802e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (626569888.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[5], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    def train\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def train\n",
    "\n",
    "\n",
    "for j, inputBatch in enumerate(train_ld):\n",
    "    X, lS_o, lS_i, T, W, CBPP = unpack_batch(inputBatch)\n",
    "\n",
    "    # early exit if nbatches was set by the user and has been exceeded\n",
    "    if nbatches > 0 and j >= nbatches:\n",
    "        break\n",
    "\n",
    "    # Skip the batch if batch size not multiple of total ranks\n",
    "    if ext_dist.my_size > 1 and X.size(0) % ext_dist.my_size != 0:\n",
    "        print(\n",
    "            \"Warning: Skiping the batch %d with size %d\"\n",
    "            % (j, X.size(0))\n",
    "        )\n",
    "        continue\n",
    "\n",
    "    mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n",
    "\n",
    "    start_time = time.time()\n",
    "    # forward pass\n",
    "    Z = dlrm_wrap(\n",
    "        X,\n",
    "        lS_o,\n",
    "        lS_i,\n",
    "        use_gpu,\n",
    "        device,\n",
    "        ndevices=ndevices,\n",
    "    )\n",
    "\n",
    "    if ext_dist.my_size > 1:\n",
    "        T = T[ext_dist.get_my_slice(mbs)]\n",
    "        W = W[ext_dist.get_my_slice(mbs)]\n",
    "\n",
    "    # loss\n",
    "    E = loss_fn_wrap(Z, T, use_gpu, device)\n",
    "\n",
    "    # compute loss and accuracy\n",
    "    L = E.detach().cpu().numpy()  # numpy array\n",
    "    # training accuracy is not disabled\n",
    "    # S = Z.detach().cpu().numpy()  # numpy array\n",
    "    # T = T.detach().cpu().numpy()  # numpy array\n",
    "\n",
    "    # # print(\"res: \", S)\n",
    "\n",
    "    # # print(\"j, train: BCE \", j, L)\n",
    "\n",
    "    # mbs = T.shape[0]  # = args.mini_batch_size except maybe for last\n",
    "    # A = np.sum((np.round(S, 0) == T).astype(np.uint8))\n",
    "    E.backward()\n",
    "\n",
    "    # optimizer\n",
    "    optimizer.step()\n",
    "    lr_scheduler.step()\n",
    "    end_time = time.time()\n",
    "\n",
    "\n",
    "    model_metrics_dict, is_best = inference(\n",
    "        args,\n",
    "        dlrm,\n",
    "        best_acc_test,\n",
    "        best_auc_test,\n",
    "        test_ld,\n",
    "        device,\n",
    "        use_gpu,\n",
    "        log_iter,\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05a142a0-bc92-4489-b914-8448c438b36f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
