{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2743831c-1cfc-4f85-9615-2ee5f2e86607",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nclass DLRM_Net(nn.Module):\\n    m_spa: int\\n    ln_emb: List[int]\\n    ln_bot: List[int]\\n    ln_top: List[int]\\n    arch_interaction_op: str\\n    arch_interaction_itself: bool = False\\n    sigmoid_bot: int = -1\\n    sigmoid_top: int = -1\\n    loss_threshold: float = 0.0\\n    weighted_pooling: Optional[str] = None\\n\\n    def setup(self):\\n        # Define embeddings as a list of embedding tables\\n        self.embeddings = [nn.Embed(num_embeddings=n, features=self.m_spa) \\n                           for n in self.ln_emb]\\n\\n        # Define MLPs\\n        self.bot_mlp = self.create_mlp(self.ln_bot, self.sigmoid_bot)\\n        self.top_mlp = self.create_mlp(self.ln_top, self.sigmoid_top)\\n\\n    def create_mlp(self, ln, sigmoid_layer):\\n        \"\"\"Helper function to create MLP layers.\"\"\"\\n        layers = []\\n        for i in range(len(ln) - 1):\\n            layers.append(nn.Dense(features=ln[i + 1]))\\n            if i == sigmoid_layer:\\n                layers.append(nn.sigmoid)\\n            else:\\n                layers.append(nn.relu)\\n        return nn.Sequential(layers)\\n\\n    def apply_embedding(self, lS_o, lS_i, embeddings):\\n        \"\"\"Embeddings lookup for sparse features using offsets.\"\"\"\\n        ly = []\\n        for k in range(len(embeddings)):\\n            E = embeddings[k]\\n            indices = lS_i[k]\\n            offsets = lS_o[k]\\n            \\n            # Perform embedding lookup using indices\\n            embeds = E(indices)\\n\\n            print(offsets.shape)\\n            \\n            # Sum over ranges defined by the offsets (as we discussed earlier)\\n            output = []\\n            for i in range(offsets.shape[0] - 1):\\n                start, end = offsets[i], offsets[i + 1]\\n                sum_embeddings = jnp.sum(embeds[start:end], axis=-1)\\n                output.append(sum_embeddings)\\n\\n            # Append the summed embeddings for each sparse feature\\n            ly.append(jnp.stack(output))\\n        \\n        return ly\\n\\n    def interact_features(self, x, ly):\\n        \"\"\"Perform feature interactions between dense and sparse features.\"\"\"\\n        if self.arch_interaction_op == \"dot\":\\n            # Concatenate dense features and sparse embeddings\\n            T = jnp.concatenate([x] + ly, axis=1).reshape(x.shape[0], -1, x.shape[1])\\n            Z = jnp.matmul(T, jnp.transpose(T, axes=(0, 2, 1)))\\n            \\n            offset = 1 if self.arch_interaction_itself else 0\\n            li = jnp.array([i for i in range(Z.shape[1]) for j in range(i + offset)])\\n            lj = jnp.array([j for i in range(Z.shape[2]) for j in range(i + offset)])\\n            \\n            Zflat = Z[:, li, lj]\\n            R = jnp.concatenate([x, Zflat], axis=1)\\n        elif self.arch_interaction_op == \"cat\":\\n            R = jnp.concatenate([x] + ly, axis=1)\\n        else:\\n            raise ValueError(f\"Unsupported interaction op: {self.arch_interaction_op}\")\\n        \\n        return R\\n\\n    def __call__(self, dense_x, lS_o, lS_i):\\n        \"\"\"Forward pass for DLRM.\"\"\"\\n        # Apply bottom MLP to dense features\\n        x = self.bot_mlp(dense_x)\\n        \\n        # Apply embedding lookup with offsets for sparse features\\n        ly = self.apply_embedding(lS_o, lS_i, self.embeddings)\\n        \\n        # Interact features between dense and sparse features\\n        z = self.interact_features(x, ly)\\n        \\n        # Apply top MLP for final prediction\\n        p = self.top_mlp(z)\\n\\n        # Optionally clip prediction based on loss threshold\\n        if 0.0 < self.loss_threshold < 1.0:\\n            p = jnp.clip(p, self.loss_threshold, 1.0 - self.loss_threshold)\\n\\n        return p\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import List, Optional\n",
    "\n",
    "'''\n",
    "class DLRM_Net(nn.Module):\n",
    "    m_spa: int\n",
    "    ln_emb: List[int]\n",
    "    ln_bot: List[int]\n",
    "    ln_top: List[int]\n",
    "    arch_interaction_op: str\n",
    "    arch_interaction_itself: bool = False\n",
    "    sigmoid_bot: int = -1\n",
    "    sigmoid_top: int = -1\n",
    "    loss_threshold: float = 0.0\n",
    "    weighted_pooling: Optional[str] = None\n",
    "\n",
    "    def setup(self):\n",
    "        # Define embeddings as a list of embedding tables\n",
    "        self.embeddings = [nn.Embed(num_embeddings=n, features=self.m_spa) \n",
    "                           for n in self.ln_emb]\n",
    "\n",
    "        # Define MLPs\n",
    "        self.bot_mlp = self.create_mlp(self.ln_bot, self.sigmoid_bot)\n",
    "        self.top_mlp = self.create_mlp(self.ln_top, self.sigmoid_top)\n",
    "\n",
    "    def create_mlp(self, ln, sigmoid_layer):\n",
    "        \"\"\"Helper function to create MLP layers.\"\"\"\n",
    "        layers = []\n",
    "        for i in range(len(ln) - 1):\n",
    "            layers.append(nn.Dense(features=ln[i + 1]))\n",
    "            if i == sigmoid_layer:\n",
    "                layers.append(nn.sigmoid)\n",
    "            else:\n",
    "                layers.append(nn.relu)\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def apply_embedding(self, lS_o, lS_i, embeddings):\n",
    "        \"\"\"Embeddings lookup for sparse features using offsets.\"\"\"\n",
    "        ly = []\n",
    "        for k in range(len(embeddings)):\n",
    "            E = embeddings[k]\n",
    "            indices = lS_i[k]\n",
    "            offsets = lS_o[k]\n",
    "            \n",
    "            # Perform embedding lookup using indices\n",
    "            embeds = E(indices)\n",
    "\n",
    "            print(offsets.shape)\n",
    "            \n",
    "            # Sum over ranges defined by the offsets (as we discussed earlier)\n",
    "            output = []\n",
    "            for i in range(offsets.shape[0] - 1):\n",
    "                start, end = offsets[i], offsets[i + 1]\n",
    "                sum_embeddings = jnp.sum(embeds[start:end], axis=-1)\n",
    "                output.append(sum_embeddings)\n",
    "\n",
    "            # Append the summed embeddings for each sparse feature\n",
    "            ly.append(jnp.stack(output))\n",
    "        \n",
    "        return ly\n",
    "\n",
    "    def interact_features(self, x, ly):\n",
    "        \"\"\"Perform feature interactions between dense and sparse features.\"\"\"\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            # Concatenate dense features and sparse embeddings\n",
    "            T = jnp.concatenate([x] + ly, axis=1).reshape(x.shape[0], -1, x.shape[1])\n",
    "            Z = jnp.matmul(T, jnp.transpose(T, axes=(0, 2, 1)))\n",
    "            \n",
    "            offset = 1 if self.arch_interaction_itself else 0\n",
    "            li = jnp.array([i for i in range(Z.shape[1]) for j in range(i + offset)])\n",
    "            lj = jnp.array([j for i in range(Z.shape[2]) for j in range(i + offset)])\n",
    "            \n",
    "            Zflat = Z[:, li, lj]\n",
    "            R = jnp.concatenate([x, Zflat], axis=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            R = jnp.concatenate([x] + ly, axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interaction op: {self.arch_interaction_op}\")\n",
    "        \n",
    "        return R\n",
    "\n",
    "    def __call__(self, dense_x, lS_o, lS_i):\n",
    "        \"\"\"Forward pass for DLRM.\"\"\"\n",
    "        # Apply bottom MLP to dense features\n",
    "        x = self.bot_mlp(dense_x)\n",
    "        \n",
    "        # Apply embedding lookup with offsets for sparse features\n",
    "        ly = self.apply_embedding(lS_o, lS_i, self.embeddings)\n",
    "        \n",
    "        # Interact features between dense and sparse features\n",
    "        z = self.interact_features(x, ly)\n",
    "        \n",
    "        # Apply top MLP for final prediction\n",
    "        p = self.top_mlp(z)\n",
    "\n",
    "        # Optionally clip prediction based on loss threshold\n",
    "        if 0.0 < self.loss_threshold < 1.0:\n",
    "            p = jnp.clip(p, self.loss_threshold, 1.0 - self.loss_threshold)\n",
    "\n",
    "        return p\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0bc9f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import List, Optional\n",
    "\n",
    "import pdb\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ln: List[int]\n",
    "    sigmoid_layer: int = -1\n",
    "    @nn.compact\n",
    "    def __call__(self,x):\n",
    "        for i in range(len(self.ln) - 1):\n",
    "            x = nn.Dense(features=self.ln[i + 1])(x)\n",
    "            if i == self.sigmoid_layer:\n",
    "                x  = nn.sigmoid(x)\n",
    "            else:\n",
    "                x = nn.relu(x)\n",
    "        return nn.Dense(features=self.ln[-1])(x)\n",
    "    \n",
    "\n",
    "\n",
    "class DLRM_Net(nn.Module):\n",
    "    m_spa: int\n",
    "    ln_emb: List[int]\n",
    "    ln_bot: List[int]\n",
    "    ln_top: List[int]\n",
    "    arch_interaction_op: str\n",
    "    arch_interaction_itself: bool = False\n",
    "    sigmoid_bot: int = -1\n",
    "    sigmoid_top: int = -1\n",
    "    loss_threshold: float = 0.0\n",
    "    weighted_pooling: Optional[str] = None\n",
    " \n",
    "    def apply_embedding(self, lS_o, lS_i, embeddings):\n",
    "        \"\"\"Embeddings lookup for sparse features using offsets.\"\"\"\n",
    "        ly = []\n",
    "        for k in range(len(embeddings)):\n",
    "            E = embeddings[k]\n",
    "            indices = lS_i[k]\n",
    "            offsets = lS_o[k]\n",
    "            \n",
    "            ## Perform embedding lookup using indices\n",
    "            embeds = E(indices)\n",
    "            #print(offsets.shape)\n",
    "            ## Sum over ranges defined by the offsets (as we discussed earlier)\n",
    "            output = []\n",
    "            for i in range(offsets.shape[0] - 1):\n",
    "                start, end = offsets[i], offsets[i + 1]\n",
    "                sum_embeddings = jnp.sum(embeds[start:end], axis=-1)\n",
    "                output.append(sum_embeddings)\n",
    "            # Append the summed embeddings for each sparse feature\n",
    "            ly.append(jnp.stack(output))\n",
    "        \n",
    "        return ly\n",
    "\n",
    "    def interact_features(self, x, ly):\n",
    "        breakpoint()\n",
    "        \"\"\"Perform feature interactions between dense and sparse features.\"\"\"\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            # Concatenate dense features and sparse embeddings\n",
    "            T = jnp.concatenate((jnp.expand_dims(x,0) , ly[0]),axis=2)#.reshape(x.shape[0], -1, x.shape[1])\n",
    "            print(T.shape)\n",
    "            \n",
    "            Z = jnp.matmul(T, jnp.transpose(T, axes=(0, 2, 1)))\n",
    "            print(Z.shape)\n",
    "            #\n",
    "            offset = 1 if self.arch_interaction_itself else 0\n",
    "            li = jnp.array([i for i in range(Z.shape[1]) for j in range(i + offset)])\n",
    "            lj = jnp.array([j for i in range(Z.shape[2]) for j in range(i + offset)])\n",
    "            #\n",
    "            Zflat = Z[:, li, lj]\n",
    "            print(x.shape)\n",
    "            print(Zflat.shape)\n",
    "            R = jnp.concatenate((x, Zflat), axis=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            R = jnp.concatenate((jnp.expand_dims(x,0) , ly[0]),axis=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interaction op: {self.arch_interaction_op}\")\n",
    "        \n",
    "        return R\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, dense_x, lS_o, lS_i):\n",
    "        \"\"\"Forward pass for DLRM.\"\"\"\n",
    "        # Apply bottom MLP to dense features\n",
    "        x = MLP(self.ln_bot, self.sigmoid_bot)(dense_x)\n",
    "        \n",
    "        embeddings = [nn.Embed(num_embeddings=n, features=self.m_spa) \n",
    "                           for n in self.ln_emb]\n",
    "    \n",
    "    \n",
    "        # Apply embedding lookup with offsets for sparse features\n",
    "        ly = self.apply_embedding(lS_o, lS_i, embeddings)\n",
    "        #\n",
    "        ## Interact features between dense and sparse features\n",
    "        z = self.interact_features(x, ly)\n",
    "        #\n",
    "        # Apply top MLP for final prediction\n",
    "        p = MLP(self.ln_top, self.sigmoid_top)(z)\n",
    "        # Optionally clip prediction based on loss threshold\n",
    "        if 0.0 < self.loss_threshold < 1.0:\n",
    "            p = jnp.clip(p, self.loss_threshold, 1.0 - self.loss_threshold)\n",
    "\n",
    "        return x,ly #p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbcad78f-43fd-4a23-83ad-3494015aa291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[ 0.27021882,  0.6180628 , -0.14204143,  0.46539277,  0.22652283,\n",
      "         0.289755  , -0.3446836 , -0.57154536,  1.0664508 ,  0.38619608,\n",
      "        -0.2058066 ,  0.08506541,  1.0736796 ,  0.12039478, -0.08093123,\n",
      "         0.23242348, -0.16108775, -0.6048284 ,  0.37975907, -1.076916  ,\n",
      "        -0.21785256,  0.432528  , -0.21895711, -0.6404866 ,  0.530484  ,\n",
      "        -0.906347  , -0.6153416 , -0.8004427 , -0.5313179 , -0.8547554 ,\n",
      "        -0.7557017 , -1.23112   ],\n",
      "       [ 0.27021882,  0.6180628 , -0.14204143,  0.46539277,  0.22652283,\n",
      "         0.289755  , -0.3446836 , -0.57154536,  1.0664508 ,  0.38619608,\n",
      "        -0.2058066 ,  0.08506541,  1.0736796 ,  0.12039478, -0.08093123,\n",
      "         0.23242348, -0.16108775, -0.6048284 ,  0.37975907, -1.076916  ,\n",
      "        -0.21785256,  0.432528  , -0.21895711, -0.6404866 ,  0.530484  ,\n",
      "        -0.906347  , -0.6153416 , -0.8004427 , -0.5313179 , -0.8547554 ,\n",
      "        -0.7557017 , -1.23112   ]], dtype=float32), [Array([[[-0.805229, -0.805229, -0.805229, -0.805229, -0.805229,\n",
      "         -0.805229, -0.805229, -0.805229, -0.805229, -0.805229],\n",
      "        [-0.805229, -0.805229, -0.805229, -0.805229, -0.805229,\n",
      "         -0.805229, -0.805229, -0.805229, -0.805229, -0.805229]]],      dtype=float32), Array([[[-1.196966, -1.196966, -1.196966, -1.196966, -1.196966,\n",
      "         -1.196966, -1.196966, -1.196966, -1.196966, -1.196966],\n",
      "        [-1.196966, -1.196966, -1.196966, -1.196966, -1.196966,\n",
      "         -1.196966, -1.196966, -1.196966, -1.196966, -1.196966]]],      dtype=float32), Array([[[0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792,\n",
      "         0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792],\n",
      "        [0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792,\n",
      "         0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792]]],      dtype=float32)])\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data Configuration\n",
    "batch_size = 2  # Batch size for testing\n",
    "num_dense_features = 10  # Number of dense features\n",
    "num_sparse_features = 3  # Number of sparse features\n",
    "num_embeddings = [20, 10, 5]  # Number of embedding entries per sparse feature\n",
    "m_spa = 8  # Size of the embedding vector\n",
    "\n",
    "# Create dummy dense features and sparse indices\n",
    "dense_x = jnp.ones((batch_size, num_dense_features))  # Dense input features\n",
    "lS_i = jnp.array([jnp.ones((batch_size, 10), dtype=int) for _ in range(num_sparse_features)])  # Sparse indices\n",
    "lS_o = jnp.array([jnp.arange(0, batch_size * 10, 10) for _ in range(num_sparse_features)])  # Sparse offsets\n",
    "\n",
    "# Model configuration\n",
    "ln_bot = [num_dense_features, 64, 32]  # Bottom MLP layers\n",
    "ln_top = [m_spa * (num_sparse_features + 1), 128, 64, 1]  # Top MLP layers (plus interaction)\n",
    "arch_interaction_op = 'cat'  # Interaction operation\n",
    "\n",
    "# Initialize the model\n",
    "model = DLRM_Net(\n",
    "    m_spa=m_spa,\n",
    "    ln_emb=num_embeddings,\n",
    "    ln_bot=ln_bot,\n",
    "    ln_top=ln_top,\n",
    "    arch_interaction_op=arch_interaction_op,\n",
    "    arch_interaction_itself=False,\n",
    "    sigmoid_bot=-1,  # No sigmoid in bottom MLP\n",
    "    sigmoid_top=len(ln_top) - 2  # Sigmoid in the last layer before output\n",
    ")\n",
    "\n",
    "# Initialize parameters using a random key\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = model.init(key, dense_x, lS_o, lS_i)\n",
    "\n",
    "test = model.apply(params, dense_x, lS_o, lS_i)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d621808-71e2-46a0-a414-0fee5f0b489d",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "We use the data for pytorch dlrm, namely the criteo advertising challenge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965254b8-6b3e-49dd-a161-1ca3b910f1ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz\n",
    "#!md5sum criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz\n",
    "# df9b1b3766d9ff91d5ca3eb3d23bed27\n",
    "#!mkdir data\n",
    "!tar -xzf criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e2aaee-09b9-4cfa-a558-ced6be5182ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment params\n",
    "num_train_trials = 5\n",
    "num_train_warmups = 1\n",
    "num_jit_trials = 10\n",
    "num_jit_warmups = 2\n",
    "num_inference_trials = 100\n",
    "num_inference_warmups = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46e218-9007-4bf2-b4be-75b66f373215",
   "metadata": {},
   "source": [
    "# JIT\n",
    "\n",
    "Run the JIT timing loop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
