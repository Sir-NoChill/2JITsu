{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bc9f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import List, Optional\n",
    "\n",
    "import pdb\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    ln: List[int]\n",
    "    sigmoid_layer: int = -1\n",
    "    @nn.compact\n",
    "    def __call__(self,x):\n",
    "        for i in range(len(self.ln) - 1):\n",
    "            x = nn.Dense(features=self.ln[i + 1])(x)\n",
    "            if i == self.sigmoid_layer:\n",
    "                x  = nn.sigmoid(x)\n",
    "            else:\n",
    "                x = nn.relu(x)\n",
    "        return nn.Dense(features=self.ln[-1])(x)\n",
    "    \n",
    "\n",
    "\n",
    "class DLRM_Net(nn.Module):\n",
    "    m_spa: int\n",
    "    ln_emb: List[int]\n",
    "    ln_bot: List[int]\n",
    "    ln_top: List[int]\n",
    "    arch_interaction_op: str\n",
    "    arch_interaction_itself: bool = False\n",
    "    sigmoid_bot: int = -1\n",
    "    sigmoid_top: int = -1\n",
    "    loss_threshold: float = 0.0\n",
    "    weighted_pooling: Optional[str] = None\n",
    " \n",
    "    def apply_embedding(self, lS_o, lS_i, embeddings):\n",
    "        \"\"\"Embeddings lookup for sparse features using offsets.\"\"\"\n",
    "        ly = []\n",
    "        for k in range(len(embeddings)):\n",
    "            E = embeddings[k]\n",
    "            indices = jnp.array(lS_i[k])\n",
    "            offsets = jnp.array(lS_o[k])\n",
    "            \n",
    "            ## Perform embedding lookup using indices\n",
    "            embeds = jnp.array(E(indices))\n",
    "            #print(offsets.shape)\n",
    "            ## Sum over ranges defined by the offsets (as we discussed earlier)\n",
    "            output = []  # This is not idiomatic JAX, replace with jnp.array([])\n",
    "            for i in range(offsets.shape[0] - 1):\n",
    "                start, end = 0, 10 #offsets.take(i), offsets.take(i + 1)\n",
    "                embed_arr = jax.lax.dynamic_slice_in_dim(embeds, start, end, axis=1)\n",
    "                sum_embeddings = jnp.sum(embed_arr, axis=-1)\n",
    "                \n",
    "                output.append(sum_embeddings)\n",
    "            # Append the summed embeddings for each sparse feature\n",
    "            ly.append(jnp.stack(output))\n",
    "            ly.append(jnp.stack(output))\n",
    "        \n",
    "        return ly\n",
    "\n",
    "    def interact_features(self, x, ly):\n",
    "        \"\"\"Perform feature interactions between dense and sparse features.\"\"\"\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            # Concatenate dense features and sparse embeddings\n",
    "            T = jnp.concatenate((jnp.expand_dims(x,0) , ly[0]),axis=2)#.reshape(x.shape[0], -1, x.shape[1])\n",
    "            print(T.shape)\n",
    "            \n",
    "            Z = jnp.matmul(T, jnp.transpose(T, axes=(0, 2, 1)))\n",
    "            print(Z.shape)\n",
    "            #\n",
    "            offset = 1 if self.arch_interaction_itself else 0\n",
    "            li = jnp.array([i for i in range(Z.shape[1]) for j in range(i + offset)])\n",
    "            lj = jnp.array([j for i in range(Z.shape[2]) for j in range(i + offset)])\n",
    "            #\n",
    "            Zflat = Z[:, li, lj]\n",
    "            print(x.shape)\n",
    "            print(Zflat.shape)\n",
    "            R = jnp.concatenate((x, Zflat), axis=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            R = jnp.concatenate((jnp.expand_dims(x,0) , ly[0]),axis=2)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interaction op: {self.arch_interaction_op}\")\n",
    "        \n",
    "        return R\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, dense_x, lS_o, lS_i):\n",
    "        \"\"\"Forward pass for DLRM.\"\"\"\n",
    "        # Apply bottom MLP to dense features\n",
    "        x = MLP(self.ln_bot, self.sigmoid_bot)(dense_x)\n",
    "        \n",
    "        embeddings = [nn.Embed(num_embeddings=n, features=self.m_spa) \n",
    "                           for n in self.ln_emb]\n",
    "    \n",
    "    \n",
    "        # Apply embedding lookup with offsets for sparse features\n",
    "        ly = self.apply_embedding(lS_o, lS_i, embeddings)\n",
    "        #\n",
    "        ## Interact features between dense and sparse features\n",
    "        z = self.interact_features(x, ly)\n",
    "        #\n",
    "        # Apply top MLP for final prediction\n",
    "        p = MLP(self.ln_top, self.sigmoid_top)(z)\n",
    "        # Optionally clip prediction based on loss threshold\n",
    "        if 0.0 < self.loss_threshold < 1.0:\n",
    "            p = jnp.clip(p, self.loss_threshold, 1.0 - self.loss_threshold)\n",
    "\n",
    "        return x,ly #p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbcad78f-43fd-4a23-83ad-3494015aa291",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-16 12:13:15.887241: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.5 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[ 0.27021882,  0.6180628 , -0.14204143,  0.46539277,  0.22652283,\n",
      "         0.289755  , -0.3446836 , -0.57154536,  1.0664508 ,  0.38619608,\n",
      "        -0.2058066 ,  0.08506541,  1.0736796 ,  0.12039478, -0.08093123,\n",
      "         0.23242348, -0.16108775, -0.6048284 ,  0.37975907, -1.076916  ,\n",
      "        -0.21785256,  0.432528  , -0.21895711, -0.6404866 ,  0.530484  ,\n",
      "        -0.906347  , -0.6153416 , -0.8004427 , -0.5313179 , -0.8547554 ,\n",
      "        -0.7557017 , -1.23112   ],\n",
      "       [ 0.27021882,  0.6180628 , -0.14204143,  0.46539277,  0.22652283,\n",
      "         0.289755  , -0.3446836 , -0.57154536,  1.0664508 ,  0.38619608,\n",
      "        -0.2058066 ,  0.08506541,  1.0736796 ,  0.12039478, -0.08093123,\n",
      "         0.23242348, -0.16108775, -0.6048284 ,  0.37975907, -1.076916  ,\n",
      "        -0.21785256,  0.432528  , -0.21895711, -0.6404866 ,  0.530484  ,\n",
      "        -0.906347  , -0.6153416 , -0.8004427 , -0.5313179 , -0.8547554 ,\n",
      "        -0.7557017 , -1.23112   ]], dtype=float32), [Array([[[-0.805229, -0.805229, -0.805229, -0.805229, -0.805229,\n",
      "         -0.805229, -0.805229, -0.805229, -0.805229, -0.805229],\n",
      "        [-0.805229, -0.805229, -0.805229, -0.805229, -0.805229,\n",
      "         -0.805229, -0.805229, -0.805229, -0.805229, -0.805229]]],      dtype=float32), Array([[[-0.805229, -0.805229, -0.805229, -0.805229, -0.805229,\n",
      "         -0.805229, -0.805229, -0.805229, -0.805229, -0.805229],\n",
      "        [-0.805229, -0.805229, -0.805229, -0.805229, -0.805229,\n",
      "         -0.805229, -0.805229, -0.805229, -0.805229, -0.805229]]],      dtype=float32), Array([[[-1.196966, -1.196966, -1.196966, -1.196966, -1.196966,\n",
      "         -1.196966, -1.196966, -1.196966, -1.196966, -1.196966],\n",
      "        [-1.196966, -1.196966, -1.196966, -1.196966, -1.196966,\n",
      "         -1.196966, -1.196966, -1.196966, -1.196966, -1.196966]]],      dtype=float32), Array([[[-1.196966, -1.196966, -1.196966, -1.196966, -1.196966,\n",
      "         -1.196966, -1.196966, -1.196966, -1.196966, -1.196966],\n",
      "        [-1.196966, -1.196966, -1.196966, -1.196966, -1.196966,\n",
      "         -1.196966, -1.196966, -1.196966, -1.196966, -1.196966]]],      dtype=float32), Array([[[0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792,\n",
      "         0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792],\n",
      "        [0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792,\n",
      "         0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792]]],      dtype=float32), Array([[[0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792,\n",
      "         0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792],\n",
      "        [0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792,\n",
      "         0.6174792, 0.6174792, 0.6174792, 0.6174792, 0.6174792]]],      dtype=float32)])\n"
     ]
    }
   ],
   "source": [
    "# Dummy Data Configuration\n",
    "batch_size = 2  # Batch size for testing\n",
    "num_dense_features = 10  # Number of dense features\n",
    "num_sparse_features = 3  # Number of sparse features\n",
    "num_embeddings = [20, 10, 5]  # Number of embedding entries per sparse feature\n",
    "m_spa = 8  # Size of the embedding vector\n",
    "\n",
    "# Create dummy dense features and sparse indices\n",
    "dense_x = jnp.ones((batch_size, num_dense_features))  # Dense input features\n",
    "lS_i = jnp.array([jnp.ones((batch_size, 10), dtype=int) for _ in range(num_sparse_features)])  # Sparse indices\n",
    "lS_o = jnp.array([jnp.arange(0, batch_size * 10, 10) for _ in range(num_sparse_features)])  # Sparse offsets\n",
    "\n",
    "# Model configuration\n",
    "ln_bot = [num_dense_features, 64, 32]  # Bottom MLP layers\n",
    "ln_top = [m_spa * (num_sparse_features + 1), 128, 64, 1]  # Top MLP layers (plus interaction)\n",
    "arch_interaction_op = 'cat'  # Interaction operation\n",
    "\n",
    "# Initialize the model\n",
    "model = DLRM_Net(\n",
    "    m_spa=m_spa,\n",
    "    ln_emb=num_embeddings,\n",
    "    ln_bot=ln_bot,\n",
    "    ln_top=ln_top,\n",
    "    arch_interaction_op=arch_interaction_op,\n",
    "    arch_interaction_itself=False,\n",
    "    sigmoid_bot=-1,  # No sigmoid in bottom MLP\n",
    "    sigmoid_top=len(ln_top) - 2  # Sigmoid in the last layer before output\n",
    ")\n",
    "\n",
    "# Initialize parameters using a random key\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = model.init(key, dense_x, lS_o, lS_i)\n",
    "\n",
    "test = model.apply(params, dense_x, lS_o, lS_i)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d621808-71e2-46a0-a414-0fee5f0b489d",
   "metadata": {},
   "source": [
    "# Get Data\n",
    "\n",
    "We use the data for pytorch dlrm, namely the criteo advertising challenge dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "965254b8-6b3e-49dd-a161-1ca3b910f1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "tar: data: Cannot open: No such file or directory\n",
      "tar: Error is not recoverable: exiting now\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    }
   ],
   "source": [
    "#!wget http://go.criteo.net/criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz\n",
    "#!md5sum criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz\n",
    "# df9b1b3766d9ff91d5ca3eb3d23bed27\n",
    "#!mkdir data\n",
    "!tar -xzf criteo-research-kaggle-display-advertising-challenge-dataset.tar.gz -C data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "89e2aaee-09b9-4cfa-a558-ced6be5182ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment params\n",
    "num_train_trials = 5\n",
    "num_train_warmups = 1\n",
    "num_jit_trials = 10\n",
    "num_jit_warmups = 2\n",
    "num_inference_trials = 10000\n",
    "num_inference_warmups = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed46e218-9007-4bf2-b4be-75b66f373215",
   "metadata": {},
   "source": [
    "# Timing\n",
    "\n",
    "Run the JIT timing loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b94471a2-bcff-4c94-97a1-6d04e7a01313",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████| 10/10 [00:01<00:00,  7.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average JIT time: 0.1318470901913113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "\n",
    "def time_jit(num_jit_runs: int, num_jit_warmups: int) -> float:\n",
    "    times = []\n",
    "    for i in tqdm.trange(1, num_jit_runs + 1):\n",
    "        start_jit_time = time.time()\n",
    "        \n",
    "        jit_model = jax.jit(model.apply, backend='gpu').lower(params, dense_x, lS_o, lS_i)\n",
    "        compiled_model = jit_model.compile()\n",
    "\n",
    "        end_jit_time = time.time()\n",
    "        jax.clear_caches()\n",
    "        if i >= num_jit_warmups:\n",
    "            times.append(end_jit_time - start_jit_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "\n",
    "average_jit_time = time_jit(num_jit_trials, num_jit_warmups)\n",
    "print(f\"Average JIT time: {average_jit_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0dcdc34b-cbdc-419d-8ac3-4d889111adda",
   "metadata": {},
   "outputs": [],
   "source": [
    "jit_model = jax.jit(model.apply, backend='gpu').lower(params, dense_x, lS_o, lS_i)\n",
    "compiled_model = jit_model.compile()\n",
    "\n",
    "res = compiled_model(params, dense_x, lS_o, lS_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9de716e9-a680-498d-9c48-1506facf24e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████| 5/5 [00:45<00:00,  9.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training time: 10.200698375701904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import optax\n",
    "\n",
    "# Same as JAX version but using model.apply().\n",
    "@jax.jit\n",
    "def mse(params, x_batched, y_batched, o, i):\n",
    "  # Define the squared loss for a single pair (x,y)\n",
    "  def squared_error(x, y, o, i):\n",
    "    pred = model.apply(params, x, o, i)\n",
    "    # TODO unsure how to calculate loss here\n",
    "    return 0.3\n",
    "  # Vectorize the previous to compute the average of the loss on all samples.\n",
    "  #return jnp.mean(jax.vmap(squared_error)(x_batched,y_batched), axis=0)\n",
    "  return squared_error(x_batched, y_batched, o, i)\n",
    "\n",
    "\n",
    "def time_train(num_train_runs: int, num_train_warmups: int, \n",
    "               params, lS_o, lS_i, res_x):\n",
    "\n",
    "    optim = optax.adam(learning_rate=0.1)\n",
    "    opt_state = optim.init(params)\n",
    "    loss_grad_fn = jax.value_and_grad(mse)\n",
    "\n",
    "    for i in tqdm.trange(num_train_trials):\n",
    "        times = []\n",
    "        start_train_time = time.time()\n",
    "    \n",
    "        for i in range(1000):\n",
    "            loss_val, grads = loss_grad_fn(params, dense_x, res_x, lS_o, lS_i)\n",
    "            updates, opt_state = optim.update(grads, opt_state)\n",
    "            params = optax.apply_updates(params, updates)\n",
    "\n",
    "        end_train_time = time.time()\n",
    "        if i >= num_train_warmups:\n",
    "            times.append(end_train_time - start_train_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "average_training_time = time_train(num_train_trials, num_train_warmups, \n",
    "                                params, lS_o, lS_i, res)\n",
    "print(f\"Average training time: {average_training_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9cf43537-a871-47b8-a708-1cbff1d22039",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████| 10000/10000 [00:00<00:00, 38468.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average inference time: 2.4001598358154297e-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def time_inference(num_inference_trials, num_inference_warmups):\n",
    "    times = []\n",
    "    for i in tqdm.trange(num_inference_trials):\n",
    "        start_inference_time = time.time()\n",
    "\n",
    "        compiled_model(params, dense_x, lS_o, lS_i)\n",
    "\n",
    "        end_inference_time = time.time()\n",
    "        if i >= num_inference_warmups:\n",
    "            times.append(end_inference_time - start_inference_time)\n",
    "\n",
    "    return sum(times) / len(times)\n",
    "\n",
    "average_inference_time = time_inference(num_inference_trials, num_inference_warmups)\n",
    "print(f\"Average inference time: {average_inference_time}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f9ecc305-0d69-4c2d-976f-16784bfd9a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook DLRM-JAX.ipynb to script\n",
      "[NbConvertApp] Writing 9975 bytes to DLRM-JAX.py\n"
     ]
    }
   ],
   "source": [
    "# export the python file\n",
    "!jupyter nbconvert --to script DLRM-JAX.ipynb\n",
    "!mv DLRM-JAX.py gen_dlrm_jax.py\n",
    "\n",
    "# write the results to the results.csv file\n",
    "import csv\n",
    "with open('results.csv', 'w', newline='') as csvfile:\n",
    "    csvwriter = csv.writer(csvfile)\n",
    "    csvwriter.writerow([\"Model\", \"Framework\", \"Evaluation\", \"Trials\", \"Warmups\", \"Time\", \"Notes\"])\n",
    "    csvwriter.writerow([\"DLRM\", \"JAX\", \"train\", \n",
    "                       num_train_trials, \n",
    "                       num_train_warmups, \n",
    "                       average_training_time, \n",
    "                       \"\"])\n",
    "    csvwriter.writerow([\"DLRM\", \"JAX\", \"inference\", \n",
    "                       num_inference_trials, \n",
    "                       num_inference_warmups, \n",
    "                       average_inference_time, \n",
    "                       \"\"])\n",
    "    csvwriter.writerow([\"DLRM\", \"JAX\", \"JIT\",\n",
    "                        num_jit_trials,\n",
    "                        num_jit_warmups,\n",
    "                        average_jit_time,\n",
    "                        \"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "625a227a-ee23-4c3e-96f4-430e9aadcfac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
