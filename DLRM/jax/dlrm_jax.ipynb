{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d7ee2e0-777a-4882-8e24-023a59264625",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.linen as nn\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "from flax.core import freeze, unfreeze\n",
    "from typing import List, Optional\n",
    "\n",
    "\n",
    "class DLRM_Net(nn.Module):\n",
    "    m_spa: int\n",
    "    ln_emb: List[int]\n",
    "    ln_bot: List[int]\n",
    "    ln_top: List[int]\n",
    "    arch_interaction_op: str\n",
    "    arch_interaction_itself: bool = False\n",
    "    sigmoid_bot: int = -1\n",
    "    sigmoid_top: int = -1\n",
    "    loss_threshold: float = 0.0\n",
    "    weighted_pooling: Optional[str] = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.embeddings = [nn.Embed(num_embeddings=n, features=self.m_spa) \n",
    "                           for n in self.ln_emb]\n",
    "\n",
    "        self.bot_mlp = self.create_mlp(self.ln_bot, self.sigmoid_bot)\n",
    "        self.top_mlp = self.create_mlp(self.ln_top, self.sigmoid_top)\n",
    "\n",
    "    def create_mlp(self, ln, sigmoid_layer):\n",
    "        layers = []\n",
    "        for i in range(len(ln) - 1):\n",
    "            layers.append(nn.Dense(features=ln[i + 1]))\n",
    "            if i == sigmoid_layer:\n",
    "                layers.append(nn.sigmoid)\n",
    "            else:\n",
    "                layers.append(nn.relu)\n",
    "        return nn.Sequential(layers)\n",
    "\n",
    "    def apply_embedding(self, lS_o, lS_i, embeddings):\n",
    "        \"\"\"Embeddings lookup for sparse features using jax.lax.gather\"\"\"\n",
    "        ly = []\n",
    "        for k in range(len(embeddings)):\n",
    "            E = embeddings[k]\n",
    "            # Using lax.gather for embedding lookup\n",
    "            gather_indices = jnp.expand_dims(lS_i[k], axis=-1)  # Shape (batch_size, 1)\n",
    "            \n",
    "            # Define the gather operation\n",
    "            gathered_embeddings = lax.gather(\n",
    "                E,\n",
    "                gather_indices,\n",
    "                dimension_numbers=lax.GatherDimensionNumbers(\n",
    "                    offset_dims=(1,),  # Offsets in the embedding dimension\n",
    "                    collapsed_slice_dims=(0,),  # Collapsing the index dimension\n",
    "                    start_index_map=(0,)  # Mapping indices to the first dimension\n",
    "                ),\n",
    "                slice_sizes=(1, 128)  # Gather 1 slice along the first dimension, and full along the second\n",
    "            )\n",
    "            \n",
    "            # Perform sum over the range of gathered embeddings specified by lS_o\n",
    "            V = jax.vmap(lambda g, o: jnp.sum(g[:o], axis=0), in_axes=(0, 0))(gathered_embeddings, lS_o[k])\n",
    "            ly.append(V)\n",
    "        \n",
    "        return ly\n",
    "\n",
    "    def interact_features(self, x, ly):\n",
    "        if self.arch_interaction_op == \"dot\":\n",
    "            T = jnp.concatenate([x] + ly, axis=1).reshape(x.shape[0], -1, x.shape[1])\n",
    "            Z = jnp.matmul(T, jnp.transpose(T, axes=(0, 2, 1)))\n",
    "            offset = 1 if self.arch_interaction_itself else 0\n",
    "            li = jnp.array([i for i in range(Z.shape[1]) for j in range(i + offset)])\n",
    "            lj = jnp.array([j for i in range(Z.shape[2]) for j in range(i + offset)])\n",
    "            Zflat = Z[:, li, lj]\n",
    "            R = jnp.concatenate([x, Zflat], axis=1)\n",
    "        elif self.arch_interaction_op == \"cat\":\n",
    "            R = jnp.concatenate([x] + ly, axis=1)\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported interaction op: {self.arch_interaction_op}\")\n",
    "        return R\n",
    "\n",
    "    def __call__(self, dense_x, lS_o, lS_i):\n",
    "        x = self.bot_mlp(dense_x)\n",
    "        ly = self.apply_embedding(lS_o, lS_i, self.embeddings)\n",
    "        z = self.interact_features(x, ly)\n",
    "        p = self.top_mlp(z)\n",
    "\n",
    "        if 0.0 < self.loss_threshold < 1.0:\n",
    "            p = jnp.clip(p, self.loss_threshold, 1.0 - self.loss_threshold)\n",
    "\n",
    "        return p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6124cc0-f2fd-4e41-ba8b-4db67b4d33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-01 03:04:30.466529: W external/xla/xla/service/gpu/nvptx_compiler.cc:893] The NVIDIA driver's CUDA version is 12.5 which is older than the PTX compiler version 12.6.68. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Invalid start_index_map; domain is [0, 0), got: 0->0.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Initialize parameters using a random key\u001b[39;00m\n\u001b[1;32m     39\u001b[0m key \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 40\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlS_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlS_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Forward pass to test the model\u001b[39;00m\n\u001b[1;32m     43\u001b[0m output \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mapply(params, dense_x, lS_o, lS_i)\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 82\u001b[0m, in \u001b[0;36mDLRM_Net.__call__\u001b[0;34m(self, dense_x, lS_o, lS_i)\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, dense_x, lS_o, lS_i):\n\u001b[1;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbot_mlp(dense_x)\n\u001b[0;32m---> 82\u001b[0m     ly \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlS_o\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlS_i\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m     z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minteract_features(x, ly)\n\u001b[1;32m     84\u001b[0m     p \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtop_mlp(z)\n",
      "    \u001b[0;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[1], line 48\u001b[0m, in \u001b[0;36mDLRM_Net.apply_embedding\u001b[0;34m(self, lS_o, lS_i, embeddings)\u001b[0m\n\u001b[1;32m     45\u001b[0m gather_indices \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mexpand_dims(lS_i[k], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Shape (batch_size, 1)\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Define the gather operation\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m gathered_embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[43m    \u001b[49m\u001b[43mE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     50\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgather_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdimension_numbers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGatherDimensionNumbers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43moffset_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Offsets in the embedding dimension\u001b[39;49;00m\n\u001b[1;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcollapsed_slice_dims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Collapsing the index dimension\u001b[39;49;00m\n\u001b[1;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstart_index_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Mapping indices to the first dimension\u001b[39;49;00m\n\u001b[1;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     56\u001b[0m \u001b[43m    \u001b[49m\u001b[43mslice_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m128\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Gather 1 slice along the first dimension, and full along the second\u001b[39;49;00m\n\u001b[1;32m     57\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;66;03m# Perform sum over the range of gathered embeddings specified by lS_o\u001b[39;00m\n\u001b[1;32m     60\u001b[0m V \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mvmap(\u001b[38;5;28;01mlambda\u001b[39;00m g, o: jnp\u001b[38;5;241m.\u001b[39msum(g[:o], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m), in_axes\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m))(gathered_embeddings, lS_o[k])\n",
      "    \u001b[0;31m[... skipping hidden 23 frame]\u001b[0m\n",
      "File \u001b[0;32m~/Code/2JITsu/venv/lib/python3.10/site-packages/jax/_src/lax/slicing.py:1510\u001b[0m, in \u001b[0;36m_gather_shape_rule\u001b[0;34m(operand, indices, dimension_numbers, slice_sizes, unique_indices, indices_are_sorted, mode, fill_value)\u001b[0m\n\u001b[1;32m   1507\u001b[0m   operand_dim_for_start_index_i \u001b[38;5;241m=\u001b[39m start_index_map[i]\n\u001b[1;32m   1508\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (operand_dim_for_start_index_i \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[1;32m   1509\u001b[0m       operand_dim_for_start_index_i \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m _rank(operand)):\n\u001b[0;32m-> 1510\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid start_index_map; domain is \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1511\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[0, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_rank(operand)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m), got: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1512\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperand_dim_for_start_index_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1514\u001b[0m _no_duplicate_dims(start_index_map, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgather\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstart_index_map\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# _is_sorted and _sorted_dims_in_range are checked in the opposite order\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;66;03m# compared to the XLA implementation. In cases when the input is not sorted\u001b[39;00m\n\u001b[1;32m   1518\u001b[0m \u001b[38;5;66;03m# AND there are problematic collapsed_slice_dims, the error message will thus\u001b[39;00m\n\u001b[1;32m   1519\u001b[0m \u001b[38;5;66;03m# be different.\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid start_index_map; domain is [0, 0), got: 0->0."
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import lax\n",
    "import numpy as np\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "\n",
    "# Dummy Data Configuration\n",
    "batch_size = 1  # Batch size for testing\n",
    "num_dense_features = 10  # Number of dense features\n",
    "num_sparse_features = 3  # Number of sparse features\n",
    "num_embeddings = [20, 10, 5]  # Number of embedding entries per sparse feature\n",
    "m_spa = 8  # Size of the embedding vector\n",
    "\n",
    "# Create dummy dense features and sparse indices\n",
    "dense_x = jnp.ones((batch_size, num_dense_features))  # Dense input features\n",
    "lS_i = [jnp.ones((batch_size, 10), dtype=int) for _ in range(num_sparse_features)]  # Sparse indices\n",
    "lS_o = [jnp.arange(0, batch_size * 10, 10) for _ in range(num_sparse_features)]  # Sparse offsets\n",
    "\n",
    "# Model configuration\n",
    "ln_bot = [num_dense_features, 64, 32]  # Bottom MLP layers\n",
    "ln_top = [m_spa * (num_sparse_features + 1), 128, 64, 1]  # Top MLP layers (plus interaction)\n",
    "arch_interaction_op = 'dot'  # Interaction operation\n",
    "\n",
    "# Initialize the model\n",
    "model = DLRM_Net(\n",
    "    m_spa=m_spa,\n",
    "    ln_emb=num_embeddings,\n",
    "    ln_bot=ln_bot,\n",
    "    ln_top=ln_top,\n",
    "    arch_interaction_op=arch_interaction_op,\n",
    "    arch_interaction_itself=False,\n",
    "    sigmoid_bot=-1,  # No sigmoid in bottom MLP\n",
    "    sigmoid_top=len(ln_top) - 2  # Sigmoid in the last layer before output\n",
    ")\n",
    "\n",
    "# Initialize parameters using a random key\n",
    "key = jax.random.PRNGKey(0)\n",
    "params = model.init(key, dense_x, lS_o, lS_i)\n",
    "\n",
    "# Forward pass to test the model\n",
    "output = model.apply(params, dense_x, lS_o, lS_i)\n",
    "\n",
    "# Print the output\n",
    "print(\"Output of the DLRM model (logits):\")\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9c3fda-9068-4b88-addf-76f1d4dbe74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
